1
00:00:00,480 --> 00:00:04,480
They call you the godfather of AI. So

2
00:00:02,960 --> 00:00:06,240
what would you be saying to people about

3
00:00:04,480 --> 00:00:07,839
their career prospects in a world of

4
00:00:06,240 --> 00:00:10,960
super intelligence? Train to be a

5
00:00:07,839 --> 00:00:13,360
plumber. Really? Yeah. Okay. I'm going

6
00:00:10,960 --> 00:00:15,519
to become a plumber. Jeffrey Hinton is

7
00:00:13,360 --> 00:00:18,000
the Nobel Prize winning pioneer whose

8
00:00:15,519 --> 00:00:20,160
groundbreaking work has shaped AI and

9
00:00:18,000 --> 00:00:21,840
the future of humanity. Why do they call

10
00:00:20,160 --> 00:00:23,680
it the godfather of AI? because there

11
00:00:21,840 --> 00:00:25,760
weren't many people who believed that we

12
00:00:23,680 --> 00:00:27,439
could model AI on the brain so that it

13
00:00:25,760 --> 00:00:29,439
learned to do complicated things like

14
00:00:27,439 --> 00:00:30,800
recognize objects and images or even do

15
00:00:29,439 --> 00:00:32,719
reasoning. And I pushed that approach

16
00:00:30,800 --> 00:00:34,160
for 50 years and then Google acquired

17
00:00:32,719 --> 00:00:35,680
that technology and I worked there for

18
00:00:34,160 --> 00:00:37,680
10 years on something that's now used

19
00:00:35,680 --> 00:00:39,760
all the time in AI. And then you left.

20
00:00:37,680 --> 00:00:41,040
Yeah. Why? So that I could talk freely

21
00:00:39,760 --> 00:00:43,840
at a conference. What did you want to

22
00:00:41,040 --> 00:00:45,760
talk about freely? How dangerous AI

23
00:00:43,840 --> 00:00:47,440
could be.

24
00:00:45,760 --> 00:00:49,120
I realized that these things will one

25
00:00:47,440 --> 00:00:50,480
day get smarter than us. And we've never

26
00:00:49,120 --> 00:00:51,760
had to deal with that. And if you want

27
00:00:50,480 --> 00:00:55,120
to know what life's like when you're not

28
00:00:51,760 --> 00:00:57,039
the apex intelligence, ask a chicken. So

29
00:00:55,120 --> 00:00:59,039
there's risks that come from people

30
00:00:57,039 --> 00:01:00,879
misusing AI. And then there's risks from

31
00:00:59,039 --> 00:01:02,399
AI getting super smart and deciding it

32
00:01:00,879 --> 00:01:03,680
doesn't need us. Is that a real risk?

33
00:01:02,399 --> 00:01:05,199
Yes, it is. But they're not going to

34
00:01:03,680 --> 00:01:06,880
stop it cuz it's too good for too many

35
00:01:05,199 --> 00:01:08,240
things. What about regulations? They

36
00:01:06,880 --> 00:01:09,840
have some, but they're not designed to

37
00:01:08,240 --> 00:01:11,600
deal with most of the threats. Like the

38
00:01:09,840 --> 00:01:13,760
European regulations have a clause that

39
00:01:11,600 --> 00:01:16,400
say none of these apply to military uses

40
00:01:13,760 --> 00:01:19,040
of AI. Really? Yeah. It's crazy. One of

41
00:01:16,400 --> 00:01:20,560
your students left OpenAI. Yeah. He was

42
00:01:19,040 --> 00:01:22,159
probably the most important person

43
00:01:20,560 --> 00:01:23,759
behind the development of the early

44
00:01:22,159 --> 00:01:25,759
versions of church GPT and I think he

45
00:01:23,759 --> 00:01:27,280
left because he had safety concerns. We

46
00:01:25,759 --> 00:01:29,040
should recognize that this stuff is an

47
00:01:27,280 --> 00:01:31,360
existential threat and we have to face

48
00:01:29,040 --> 00:01:34,799
the possibility that unless we do

49
00:01:31,360 --> 00:01:36,240
something soon we're near the end. So

50
00:01:34,799 --> 00:01:39,600
let's do the risks. What do we end up

51
00:01:36,240 --> 00:01:41,200
doing in such a world?

52
00:01:39,600 --> 00:01:43,840
This has always blown my mind a little

53
00:01:41,200 --> 00:01:46,000
bit. 53% of you that listen to the show

54
00:01:43,840 --> 00:01:47,840
regularly haven't yet subscribed to the

55
00:01:46,000 --> 00:01:49,119
show. So, could I ask you for a favor

56
00:01:47,840 --> 00:01:50,320
before we start? If you like the show

57
00:01:49,119 --> 00:01:52,000
and you like what we do here and you

58
00:01:50,320 --> 00:01:53,520
want to support us, the free simple way

59
00:01:52,000 --> 00:01:55,360
that you can do just that is by hitting

60
00:01:53,520 --> 00:01:57,200
the subscribe button. And my commitment

61
00:01:55,360 --> 00:01:58,880
to you is if you do that, then I'll do

62
00:01:57,200 --> 00:02:00,240
everything in my power, me and my team,

63
00:01:58,880 --> 00:02:02,159
to make sure that this show is better

64
00:02:00,240 --> 00:02:03,920
for you every single week. We'll listen

65
00:02:02,159 --> 00:02:05,759
to your feedback. We'll find the guests

66
00:02:03,920 --> 00:02:07,439
that you want me to speak to and we'll

67
00:02:05,759 --> 00:02:10,439
continue to do what we do. Thank you so

68
00:02:07,439 --> 00:02:10,439
much.

69
00:02:11,360 --> 00:02:16,319
Jeffrey Hinsson, they call you the

70
00:02:14,400 --> 00:02:18,560
godfather of AI.

71
00:02:16,319 --> 00:02:21,599
Uh yes they do. Why do they call you

72
00:02:18,560 --> 00:02:23,200
that? There weren't that many people who

73
00:02:21,599 --> 00:02:24,879
believed that we could make neural

74
00:02:23,200 --> 00:02:27,840
networks work, artificial neural

75
00:02:24,879 --> 00:02:31,120
networks. So for a long time in AI from

76
00:02:27,840 --> 00:02:34,160
the 1950s onwards, there were kind of

77
00:02:31,120 --> 00:02:36,879
two ideas about how to do AI.

78
00:02:34,160 --> 00:02:39,360
One idea was that sort of core of human

79
00:02:36,879 --> 00:02:40,959
intelligence was reasoning. And to do

80
00:02:39,360 --> 00:02:43,680
reasoning, you needed to use some form

81
00:02:40,959 --> 00:02:47,200
of logic. And so AI had to be based

82
00:02:43,680 --> 00:02:48,720
around logic. And in your head, you must

83
00:02:47,200 --> 00:02:50,879
have something like symbolic expressions

84
00:02:48,720 --> 00:02:53,200
that you manipulated with rules. And

85
00:02:50,879 --> 00:02:55,120
that's how intelligence worked. And

86
00:02:53,200 --> 00:02:57,120
things like learning or reasoning by

87
00:02:55,120 --> 00:02:59,599
analogy, that all come later once we've

88
00:02:57,120 --> 00:03:01,200
figured out how basic reasoning works.

89
00:02:59,599 --> 00:03:04,879
There was a different approach, which is

90
00:03:01,200 --> 00:03:06,800
to say, let's model AI on the brain

91
00:03:04,879 --> 00:03:10,400
because obviously the brain makes us

92
00:03:06,800 --> 00:03:13,120
intelligent. So simulate a network of

93
00:03:10,400 --> 00:03:14,959
brain cells on a computer and try and

94
00:03:13,120 --> 00:03:17,040
figure out how you would learn strengths

95
00:03:14,959 --> 00:03:19,360
of connections between brain cells so

96
00:03:17,040 --> 00:03:21,280
that it learned to do complicated things

97
00:03:19,360 --> 00:03:24,720
like recognize objects in images or

98
00:03:21,280 --> 00:03:27,120
recognize speech or even do reasoning. I

99
00:03:24,720 --> 00:03:30,319
pushed that approach for like 50 years

100
00:03:27,120 --> 00:03:32,080
because so few people believed in it.

101
00:03:30,319 --> 00:03:35,360
There weren't many good universities

102
00:03:32,080 --> 00:03:37,760
that had groups that did that. So if you

103
00:03:35,360 --> 00:03:39,040
did that the best young students who

104
00:03:37,760 --> 00:03:41,200
believed in that came and worked with

105
00:03:39,040 --> 00:03:44,159
you. So I was very fortunate in getting

106
00:03:41,200 --> 00:03:46,400
a whole lot of really good students some

107
00:03:44,159 --> 00:03:48,159
of which have gone on to create and play

108
00:03:46,400 --> 00:03:52,239
an instrumental role in creating

109
00:03:48,159 --> 00:03:54,480
platforms like open AI. Yes. So I sus

110
00:03:52,239 --> 00:03:57,200
a nice example a whole bunch of them.

111
00:03:54,480 --> 00:03:59,599
Why did you believe that modeling it off

112
00:03:57,200 --> 00:04:02,080
the brain was a more effective approach?

113
00:03:59,599 --> 00:04:04,959
It wasn't just me believed it early on.

114
00:04:02,080 --> 00:04:07,120
Fonoyman believed it and Cheuring

115
00:04:04,959 --> 00:04:08,959
believed it and if either of those had

116
00:04:07,120 --> 00:04:10,720
lived I think AI would have had a very

117
00:04:08,959 --> 00:04:13,200
different history but they both died

118
00:04:10,720 --> 00:04:15,760
young. You think AI would have been here

119
00:04:13,200 --> 00:04:18,639
sooner? I think neural net the neural

120
00:04:15,760 --> 00:04:20,799
net approach would have been accepted

121
00:04:18,639 --> 00:04:23,440
much sooner if either of them had lived

122
00:04:20,799 --> 00:04:26,320
in this season of your life. What

123
00:04:23,440 --> 00:04:29,600
mission are you on? My main mission now

124
00:04:26,320 --> 00:04:33,040
is to warn people how dangerous AI could

125
00:04:29,600 --> 00:04:36,400
be. Did you know that when you became

126
00:04:33,040 --> 00:04:38,960
the godfather of AI? No, not really. I

127
00:04:36,400 --> 00:04:40,320
was quite slow to understand some of the

128
00:04:38,960 --> 00:04:42,240
risks. Some of the risks were always

129
00:04:40,320 --> 00:04:44,880
very obvious, like people would use AI

130
00:04:42,240 --> 00:04:46,560
to make autonomous lethal weapons. That

131
00:04:44,880 --> 00:04:49,040
is things that go around deciding by

132
00:04:46,560 --> 00:04:50,720
themselves who to kill. Other risks,

133
00:04:49,040 --> 00:04:53,759
like the idea that they would one day

134
00:04:50,720 --> 00:04:56,080
get smarter than us and maybe would

135
00:04:53,759 --> 00:04:58,000
become irrelevant, I was slow to

136
00:04:56,080 --> 00:05:01,199
recognize that. Other people recognized

137
00:04:58,000 --> 00:05:03,360
it 20 years ago. I only recognized it a

138
00:05:01,199 --> 00:05:04,880
few years ago that that was a real risk

139
00:05:03,360 --> 00:05:07,520
that was come might be coming quite

140
00:05:04,880 --> 00:05:10,240
soon. How could you not have foreseen

141
00:05:07,520 --> 00:05:12,479
that if if with everything you know here

142
00:05:10,240 --> 00:05:14,560
about cracking the ability for these

143
00:05:12,479 --> 00:05:17,039
computers to learn similar to how humans

144
00:05:14,560 --> 00:05:19,360
learn and just you know introducing any

145
00:05:17,039 --> 00:05:20,960
rate of improvement? It's a very good

146
00:05:19,360 --> 00:05:24,479
question. How could you not have seen

147
00:05:20,960 --> 00:05:27,440
that? But remember neural networks 20 30

148
00:05:24,479 --> 00:05:28,880
years ago were very primitive in what

149
00:05:27,440 --> 00:05:31,120
they could do. They were nowhere near as

150
00:05:28,880 --> 00:05:33,840
good as humans, but things like vision

151
00:05:31,120 --> 00:05:35,440
and language and speech recognition. The

152
00:05:33,840 --> 00:05:36,880
idea that you have to now worry about it

153
00:05:35,440 --> 00:05:39,680
getting smarter than people, that seems

154
00:05:36,880 --> 00:05:41,840
silly then. When did that change? It

155
00:05:39,680 --> 00:05:46,000
changed for the general population when

156
00:05:41,840 --> 00:05:48,800
chat GPT came out. It changed for me

157
00:05:46,000 --> 00:05:51,440
when I realized that the kinds of

158
00:05:48,800 --> 00:05:53,120
digital intelligences we're making have

159
00:05:51,440 --> 00:05:54,720
something that makes them far superior

160
00:05:53,120 --> 00:05:57,039
to the kind of biological intelligence

161
00:05:54,720 --> 00:05:59,280
we have. If I want to share information

162
00:05:57,039 --> 00:06:01,199
with you, so I go off and I learn

163
00:05:59,280 --> 00:06:04,319
something and I'd like to tell you what

164
00:06:01,199 --> 00:06:06,000
I learned. So I produce some sentences.

165
00:06:04,319 --> 00:06:07,840
This is a rather simplistic model, but

166
00:06:06,000 --> 00:06:09,280
roughly right. Your brain is trying to

167
00:06:07,840 --> 00:06:10,720
figure out how can I change the strength

168
00:06:09,280 --> 00:06:12,560
of connections between neurons. So I

169
00:06:10,720 --> 00:06:14,000
might have put that word next. And so

170
00:06:12,560 --> 00:06:15,919
you'll do a lot of learning when a very

171
00:06:14,000 --> 00:06:17,520
surprising word comes and not much

172
00:06:15,919 --> 00:06:19,919
learning when if it's when it's very

173
00:06:17,520 --> 00:06:21,520
obvious word. If I say fish and chips,

174
00:06:19,919 --> 00:06:23,600
you don't do much learning when I say

175
00:06:21,520 --> 00:06:25,120
chips. But if I say fish and cucumber,

176
00:06:23,600 --> 00:06:27,520
you do a lot more learning. You wonder

177
00:06:25,120 --> 00:06:29,360
why did I say cucumber? So that's

178
00:06:27,520 --> 00:06:31,440
roughly what's going on in your brain.

179
00:06:29,360 --> 00:06:33,440
I'm predicting what's coming next.

180
00:06:31,440 --> 00:06:35,039
That's how we think it's working. Nobody

181
00:06:33,440 --> 00:06:37,680
really knows for sure how the brain

182
00:06:35,039 --> 00:06:39,280
works. And nobody knows how it gets the

183
00:06:37,680 --> 00:06:40,800
information about whether you should

184
00:06:39,280 --> 00:06:42,400
increase the strength of a connection or

185
00:06:40,800 --> 00:06:44,800
decrease the strength of a connection.

186
00:06:42,400 --> 00:06:47,120
That's the crucial thing. But what we do

187
00:06:44,800 --> 00:06:49,919
know now from AI

188
00:06:47,120 --> 00:06:51,600
is that if you could get information

189
00:06:49,919 --> 00:06:53,600
about whether to increase or decrease

190
00:06:51,600 --> 00:06:55,039
the connection strength so as to do

191
00:06:53,600 --> 00:06:57,840
better at whatever task you're trying to

192
00:06:55,039 --> 00:06:58,880
do, then we could learn incredible

193
00:06:57,840 --> 00:07:01,520
things because that's what we're doing

194
00:06:58,880 --> 00:07:03,120
now with artificial neuronets.

195
00:07:01,520 --> 00:07:04,560
It's just we don't know for real brains

196
00:07:03,120 --> 00:07:06,880
how they get that signal about whether

197
00:07:04,560 --> 00:07:08,560
to increase or decrease.

198
00:07:06,880 --> 00:07:10,880
As we sit here today, what are the big

199
00:07:08,560 --> 00:07:14,160
concerns you have around safety of AI?

200
00:07:10,880 --> 00:07:15,440
if we were to to list the the top couple

201
00:07:14,160 --> 00:07:17,520
that are really front of mind and that

202
00:07:15,440 --> 00:07:19,280
we should be thinking about. Um, can I

203
00:07:17,520 --> 00:07:20,720
have more than a couple? Go ahead. I'll

204
00:07:19,280 --> 00:07:22,880
write them all down and we'll go through

205
00:07:20,720 --> 00:07:25,280
them. Okay. First of all, I want to make

206
00:07:22,880 --> 00:07:27,680
a distinction between two completely

207
00:07:25,280 --> 00:07:30,080
different kinds of risk.

208
00:07:27,680 --> 00:07:32,880
There's risks that come from people

209
00:07:30,080 --> 00:07:35,199
misusing AI. Yeah. And that's most of

210
00:07:32,880 --> 00:07:37,919
the risks and all of the short-term

211
00:07:35,199 --> 00:07:40,319
risks. And then there's risks that come

212
00:07:37,919 --> 00:07:43,520
from AI getting super smart and deciding

213
00:07:40,319 --> 00:07:45,919
it doesn't need us. Is that a real risk?

214
00:07:43,520 --> 00:07:47,840
And I talk mainly about that second risk

215
00:07:45,919 --> 00:07:51,280
because lots of people say, "Is that a

216
00:07:47,840 --> 00:07:52,800
real risk?" And yes, it is. Now, we

217
00:07:51,280 --> 00:07:54,160
don't know how much of a risk it is.

218
00:07:52,800 --> 00:07:55,680
We've never been in that situation

219
00:07:54,160 --> 00:07:58,639
before. We've never had to deal with

220
00:07:55,680 --> 00:08:01,280
things smarter than us. So really, the

221
00:07:58,639 --> 00:08:04,160
thing about that existential threat is

222
00:08:01,280 --> 00:08:06,080
that we have no idea how to deal with

223
00:08:04,160 --> 00:08:07,840
it. We have no idea what it's going to

224
00:08:06,080 --> 00:08:08,800
look like. And anybody who tells you

225
00:08:07,840 --> 00:08:10,479
they know just what's going to happen

226
00:08:08,800 --> 00:08:12,000
and how to deal with it, they're talking

227
00:08:10,479 --> 00:08:14,319
nonsense. So, we don't know how to

228
00:08:12,000 --> 00:08:17,120
estimate the probabil probabilities

229
00:08:14,319 --> 00:08:19,199
it'll replace us. Um, some people say

230
00:08:17,120 --> 00:08:21,840
it's like less than 1%. My friend Yan

231
00:08:19,199 --> 00:08:24,639
Lar who was a postto with me thinks no

232
00:08:21,840 --> 00:08:25,680
no no, we're always going to be we build

233
00:08:24,639 --> 00:08:27,759
these things. We're always going to be

234
00:08:25,680 --> 00:08:29,360
in control. We'll build them to be

235
00:08:27,759 --> 00:08:33,760
obedient.

236
00:08:29,360 --> 00:08:35,120
And other people like Yudkowski say,

237
00:08:33,760 --> 00:08:36,719
"No, no, no. These things are going to

238
00:08:35,120 --> 00:08:38,800
wipe us out for sure. If anybody builds

239
00:08:36,719 --> 00:08:41,279
it, it's going to wipe us all out." And

240
00:08:38,800 --> 00:08:43,760
he's confident of that. I think both of

241
00:08:41,279 --> 00:08:45,200
those positions are extreme. It's very

242
00:08:43,760 --> 00:08:48,080
hard to estimate the probabilities in

243
00:08:45,200 --> 00:08:51,680
between. If you had to bet on who was

244
00:08:48,080 --> 00:08:54,160
right out of your two friends,

245
00:08:51,680 --> 00:08:55,440
I simply don't know. So, if I had to

246
00:08:54,160 --> 00:08:57,279
bet, I'd say the probabilities in

247
00:08:55,440 --> 00:08:59,360
between, and I don't know where to

248
00:08:57,279 --> 00:09:02,160
estimate it in between. I often say 10

249
00:08:59,360 --> 00:09:05,519
to 20% chance they'll wipe us out, but

250
00:09:02,160 --> 00:09:07,519
that's just gut based on the idea that

251
00:09:05,519 --> 00:09:10,640
we're we're still making them and we're

252
00:09:07,519 --> 00:09:12,160
pretty ingenious. And the hope is that

253
00:09:10,640 --> 00:09:14,480
if enough smart people do enough

254
00:09:12,160 --> 00:09:15,839
research with enough resources, we'll

255
00:09:14,480 --> 00:09:19,440
figure out a way to build them so

256
00:09:15,839 --> 00:09:21,519
they'll never want to harm us. Sometimes

257
00:09:19,440 --> 00:09:23,360
I think if we we talk about that second

258
00:09:21,519 --> 00:09:25,279
um path, sometimes I think about nuclear

259
00:09:23,360 --> 00:09:27,600
bombs and the the invention of the

260
00:09:25,279 --> 00:09:29,040
atomic bomb and how it compares like how

261
00:09:27,600 --> 00:09:30,240
is this different because the atomic

262
00:09:29,040 --> 00:09:31,920
bomb came along and I imagine a lot of

263
00:09:30,240 --> 00:09:35,120
people at that time thought our days are

264
00:09:31,920 --> 00:09:37,519
numbered. Yes, I was there. We did.

265
00:09:35,120 --> 00:09:40,880
Yeah. But but but what's what h we're

266
00:09:37,519 --> 00:09:42,720
still here. We're still here. Yes. So

267
00:09:40,880 --> 00:09:45,680
the atomic bomb was really only good for

268
00:09:42,720 --> 00:09:47,440
one thing and it was very obvious how it

269
00:09:45,680 --> 00:09:50,160
worked. Even if you hadn't had the

270
00:09:47,440 --> 00:09:53,040
pictures of Hiroshima and Nagasaki, it

271
00:09:50,160 --> 00:09:56,800
was obvious that it was a very big bomb

272
00:09:53,040 --> 00:10:00,480
that was very dangerous. With AI,

273
00:09:56,800 --> 00:10:02,000
it's good for many, many things. It's

274
00:10:00,480 --> 00:10:04,160
going to be magnificent in healthcare

275
00:10:02,000 --> 00:10:07,680
and education and more or less any

276
00:10:04,160 --> 00:10:08,880
industry that needs to use its data is

277
00:10:07,680 --> 00:10:11,440
going to be able to use it better with

278
00:10:08,880 --> 00:10:13,760
AI. So, we're not going to stop the

279
00:10:11,440 --> 00:10:14,959
development.

280
00:10:13,760 --> 00:10:17,200
You know, people say, "Well, why don't

281
00:10:14,959 --> 00:10:19,200
we just stop it now?" We're not going to

282
00:10:17,200 --> 00:10:21,360
stop it because it's too good for too

283
00:10:19,200 --> 00:10:22,640
many things. Also, we're not going to

284
00:10:21,360 --> 00:10:25,120
stop it because it's good for battle

285
00:10:22,640 --> 00:10:27,279
robots, and none of the countries that

286
00:10:25,120 --> 00:10:30,399
sell weapons are going to want to stop

287
00:10:27,279 --> 00:10:31,920
it. Like the European regulations, they

288
00:10:30,399 --> 00:10:33,920
have some regulations about AI, and it's

289
00:10:31,920 --> 00:10:35,279
good they have some regulations, but

290
00:10:33,920 --> 00:10:38,000
they're not designed to deal with most

291
00:10:35,279 --> 00:10:40,240
of the threats. And in particular, the

292
00:10:38,000 --> 00:10:42,640
European regulations have a a clause in

293
00:10:40,240 --> 00:10:45,839
them that say none of these regulations

294
00:10:42,640 --> 00:10:47,920
apply to military uses of AI.

295
00:10:45,839 --> 00:10:50,560
So governments are willing to regulate

296
00:10:47,920 --> 00:10:51,600
regulate companies and people, but

297
00:10:50,560 --> 00:10:53,519
they're not willing to regulate

298
00:10:51,600 --> 00:10:56,160
themselves.

299
00:10:53,519 --> 00:10:58,320
It seems pretty crazy to me that they I

300
00:10:56,160 --> 00:10:59,600
go back and forward, but if Europe has a

301
00:10:58,320 --> 00:11:01,600
regulation, but the rest of the world

302
00:10:59,600 --> 00:11:03,680
doesn't

303
00:11:01,600 --> 00:11:04,560
competitive disadvantage. Yeah, we're

304
00:11:03,680 --> 00:11:06,240
seeing this already. I don't think

305
00:11:04,560 --> 00:11:08,480
people realize that when OpenAI release

306
00:11:06,240 --> 00:11:11,120
a new model or a new piece of software

307
00:11:08,480 --> 00:11:12,720
in America, they can't release it to

308
00:11:11,120 --> 00:11:15,440
Europe yet because of regulations here.

309
00:11:12,720 --> 00:11:16,959
So Sam Alman tweeted saying, "Our new AI

310
00:11:15,440 --> 00:11:18,720
agent thing is available to everybody,

311
00:11:16,959 --> 00:11:20,480
but it can't come to Europe yet because

312
00:11:18,720 --> 00:11:22,079
there's regulations."

313
00:11:20,480 --> 00:11:24,000
Yes. What does that gives us a

314
00:11:22,079 --> 00:11:27,279
productive disadvantage? Productivity

315
00:11:24,000 --> 00:11:29,200
disadvantage. What we need is I mean at

316
00:11:27,279 --> 00:11:30,720
this point in history when we're about

317
00:11:29,200 --> 00:11:34,640
to produce things more intelligent than

318
00:11:30,720 --> 00:11:36,320
ourselves, what we really need is a kind

319
00:11:34,640 --> 00:11:38,320
of world government that works run by

320
00:11:36,320 --> 00:11:40,640
intelligent, thoughtful people. And

321
00:11:38,320 --> 00:11:43,839
that's not what we got.

322
00:11:40,640 --> 00:11:47,200
So free-for-all. Well, that what we've

323
00:11:43,839 --> 00:11:49,680
got is sort of

324
00:11:47,200 --> 00:11:51,760
we've got capitalism which is done very

325
00:11:49,680 --> 00:11:55,440
nicely by us. is produce lots of goods

326
00:11:51,760 --> 00:11:58,320
goods and services for us. But these big

327
00:11:55,440 --> 00:12:01,519
companies, they're legally required to

328
00:11:58,320 --> 00:12:03,200
try and maximize profits and that's not

329
00:12:01,519 --> 00:12:04,959
what you want from the people developing

330
00:12:03,200 --> 00:12:06,560
this stuff.

331
00:12:04,959 --> 00:12:08,240
So let's do the risks then. You talked

332
00:12:06,560 --> 00:12:09,839
about there's human risks and then

333
00:12:08,240 --> 00:12:11,600
there's So I've distinguished these two

334
00:12:09,839 --> 00:12:15,279
kinds of risk. Let's talk about all the

335
00:12:11,600 --> 00:12:18,399
risks from bad human actors using AI.

336
00:12:15,279 --> 00:12:22,639
There's cyber attacks.

337
00:12:18,399 --> 00:12:24,480
So between 2023 and 2024,

338
00:12:22,639 --> 00:12:27,480
they increased by about a factor of

339
00:12:24,480 --> 00:12:27,480
12,200%.

340
00:12:27,519 --> 00:12:32,079
And that's probably because these large

341
00:12:29,839 --> 00:12:34,480
language models make it much easier to

342
00:12:32,079 --> 00:12:36,639
do fishing attacks. And a fishing attack

343
00:12:34,480 --> 00:12:39,760
for anyone that doesn't know is it's

344
00:12:36,639 --> 00:12:41,839
they send you something saying, uh, hi,

345
00:12:39,760 --> 00:12:43,600
I'm your friend John and I'm stuck in El

346
00:12:41,839 --> 00:12:45,839
Salvador. Could you just wire this

347
00:12:43,600 --> 00:12:47,360
money? That's one kind of attack. But

348
00:12:45,839 --> 00:12:49,920
the fishing attacks are really trying to

349
00:12:47,360 --> 00:12:52,000
get your loon credentials. And now with

350
00:12:49,920 --> 00:12:53,600
AI, they can clone my voice, my image.

351
00:12:52,000 --> 00:12:55,120
They can do all that. I'm struggling at

352
00:12:53,600 --> 00:12:57,680
the moment because there's a bunch of AI

353
00:12:55,120 --> 00:12:59,760
scams on X and also Meta. And there's

354
00:12:57,680 --> 00:13:01,040
one in particular on Meta, so Instagram,

355
00:12:59,760 --> 00:13:03,200
Facebook at the moment, which is a paid

356
00:13:01,040 --> 00:13:04,800
advert where they've taken my voice from

357
00:13:03,200 --> 00:13:06,320
the podcast. They've taken the my

358
00:13:04,800 --> 00:13:08,800
mannerisms and they've made a new video

359
00:13:06,320 --> 00:13:11,279
of me encouraging people to go and take

360
00:13:08,800 --> 00:13:13,120
part in this crypto Ponzi scam or

361
00:13:11,279 --> 00:13:14,160
whatever. And we've been, you know, we

362
00:13:13,120 --> 00:13:15,920
spent weeks and weeks and weeks and

363
00:13:14,160 --> 00:13:17,200
weeks and end emailing Meta telling,

364
00:13:15,920 --> 00:13:18,800
"Please take this down." They take it

365
00:13:17,200 --> 00:13:20,160
down, another one pops up. They take

366
00:13:18,800 --> 00:13:21,760
that one down, another one pops up. So,

367
00:13:20,160 --> 00:13:23,200
it's like whack-a-ole. And then it's

368
00:13:21,760 --> 00:13:24,399
very annoying. The the heartbreaking

369
00:13:23,200 --> 00:13:26,000
part is you get the messages from people

370
00:13:24,399 --> 00:13:28,320
that have fallen for the scam and

371
00:13:26,000 --> 00:13:30,000
they've lost £500 or $500 and they cross

372
00:13:28,320 --> 00:13:32,079
with you cuz you recommended it and I'm

373
00:13:30,000 --> 00:13:34,240
I'm like I'm sad for them. It's very

374
00:13:32,079 --> 00:13:36,320
annoying. Yeah. I have a a smaller

375
00:13:34,240 --> 00:13:39,120
version of that which is PE some people

376
00:13:36,320 --> 00:13:41,839
now publish papers with me as one of the

377
00:13:39,120 --> 00:13:43,440
authors. Mhm. And it looks like it's in

378
00:13:41,839 --> 00:13:46,639
order that they can get lots of

379
00:13:43,440 --> 00:13:48,000
citations to themselves. Ah, so cyber

380
00:13:46,639 --> 00:13:51,360
attacks a very real threat. There's been

381
00:13:48,000 --> 00:13:53,600
an explosion of those. And these already

382
00:13:51,360 --> 00:13:56,000
obviously AI is very patient. So they

383
00:13:53,600 --> 00:13:57,760
can go through 100 million lines of code

384
00:13:56,000 --> 00:14:00,480
looking for known ways of attacking

385
00:13:57,760 --> 00:14:02,800
them. That's easy to do. But they're

386
00:14:00,480 --> 00:14:06,720
going to get more creative and they may

387
00:14:02,800 --> 00:14:09,199
some people believe and I some people

388
00:14:06,720 --> 00:14:12,160
who know a lot believe that maybe by

389
00:14:09,199 --> 00:14:15,360
2030 they'll be creating new kinds of

390
00:14:12,160 --> 00:14:18,480
cyber attacks which no person ever

391
00:14:15,360 --> 00:14:19,600
thought of. So that's very worrisome

392
00:14:18,480 --> 00:14:20,560
because they can think for themselves

393
00:14:19,600 --> 00:14:22,800
and discover they can think for

394
00:14:20,560 --> 00:14:24,639
themselves. They can draw new

395
00:14:22,800 --> 00:14:26,720
conclusions from much more data than a

396
00:14:24,639 --> 00:14:29,199
person ever saw. Is there anything

397
00:14:26,720 --> 00:14:32,000
you're doing to protect yourself from

398
00:14:29,199 --> 00:14:34,800
cyber attacks at all? Yes. It's one of

399
00:14:32,000 --> 00:14:36,880
the few places where I changed what I do

400
00:14:34,800 --> 00:14:39,600
radically because I'm scared of cyber

401
00:14:36,880 --> 00:14:43,120
attacks. Canadian banks are extremely

402
00:14:39,600 --> 00:14:45,360
safe. In 2008, no Canadian banks came

403
00:14:43,120 --> 00:14:46,800
anywhere near going bust. So, they're

404
00:14:45,360 --> 00:14:49,360
very safe banks because they're well

405
00:14:46,800 --> 00:14:51,839
regulated, fairly well regulated.

406
00:14:49,360 --> 00:14:54,880
Nevertheless, I think a cyber attack

407
00:14:51,839 --> 00:14:57,199
might be able to bring down a bank. Now,

408
00:14:54,880 --> 00:15:01,600
if you have all my savings are in shares

409
00:14:57,199 --> 00:15:04,480
in banks held by banks, so if the bank

410
00:15:01,600 --> 00:15:07,199
gets attacked and it holds your shares,

411
00:15:04,480 --> 00:15:11,120
they're still your shares. And so, I

412
00:15:07,199 --> 00:15:12,560
think you'd be okay unless the attacker

413
00:15:11,120 --> 00:15:15,040
sells the shares because the bank can

414
00:15:12,560 --> 00:15:18,480
sell the shares. If the attacker sells

415
00:15:15,040 --> 00:15:20,560
your shares, I think you're screwed. I

416
00:15:18,480 --> 00:15:21,760
don't know. I mean, maybe the bank would

417
00:15:20,560 --> 00:15:24,160
have to try and reimburse you, but the

418
00:15:21,760 --> 00:15:26,240
bank's bust by now, right? So,

419
00:15:24,160 --> 00:15:29,199
So I'm worried about a Canadian bank

420
00:15:26,240 --> 00:15:31,279
being taken down by a cyber attack and

421
00:15:29,199 --> 00:15:34,560
the attacker selling selling shares that

422
00:15:31,279 --> 00:15:37,600
it holds. So I spread my money and my

423
00:15:34,560 --> 00:15:39,839
children's money between three banks in

424
00:15:37,600 --> 00:15:42,079
the belief that if a cyber attack takes

425
00:15:39,839 --> 00:15:44,639
down one Canadian bank, the other

426
00:15:42,079 --> 00:15:47,199
Canadian banks will very quickly get

427
00:15:44,639 --> 00:15:49,040
very careful. And do you have a phone

428
00:15:47,199 --> 00:15:49,920
that's not connected to the internet? Do

429
00:15:49,040 --> 00:15:51,199
you have any like, you know, I'm

430
00:15:49,920 --> 00:15:53,120
thinking about storing data and stuff

431
00:15:51,199 --> 00:15:56,639
like that. Do you think it's wise to

432
00:15:53,120 --> 00:15:58,399
consider having cold storage? I have a

433
00:15:56,639 --> 00:16:01,519
little disc drive and I back up my

434
00:15:58,399 --> 00:16:03,519
laptop on this hard drive. So I actually

435
00:16:01,519 --> 00:16:05,839
have everything on my laptop on a hard

436
00:16:03,519 --> 00:16:07,680
drive. At least you know if the whole

437
00:16:05,839 --> 00:16:09,440
internet went down I had the sense I

438
00:16:07,680 --> 00:16:13,199
still got it on my laptop and I still

439
00:16:09,440 --> 00:16:16,880
got my information. Okay. Then the next

440
00:16:13,199 --> 00:16:18,639
thing is using AI to create nasty

441
00:16:16,880 --> 00:16:22,320
viruses.

442
00:16:18,639 --> 00:16:25,360
Okay. And the problem with that is that

443
00:16:22,320 --> 00:16:27,360
just requires one crazy guy with the

444
00:16:25,360 --> 00:16:29,839
grudge. One guy who knows a little bit

445
00:16:27,360 --> 00:16:33,440
of molecular biology, knows a lot about

446
00:16:29,839 --> 00:16:35,920
AI, and just wants to destroy the world.

447
00:16:33,440 --> 00:16:39,839
You can now create

448
00:16:35,920 --> 00:16:41,440
new viruses relatively cheaply using AI.

449
00:16:39,839 --> 00:16:43,360
And you don't have to be a very skilled

450
00:16:41,440 --> 00:16:44,959
molecular biologist to do it. And that's

451
00:16:43,360 --> 00:16:47,440
very scary. So you could have a small

452
00:16:44,959 --> 00:16:49,360
cult, for example.

453
00:16:47,440 --> 00:16:51,759
a small cult might be able to raise a

454
00:16:49,360 --> 00:16:53,199
few million dollars. For a few million

455
00:16:51,759 --> 00:16:54,880
dollars, they might be able to design a

456
00:16:53,199 --> 00:16:56,079
whole bunch of viruses. Well, I'm

457
00:16:54,880 --> 00:16:58,079
thinking about some of our foreign

458
00:16:56,079 --> 00:16:59,279
adversaries doing government funded

459
00:16:58,079 --> 00:17:01,120
programs. I mean, there was lots of talk

460
00:16:59,279 --> 00:17:02,320
around COVID and Woo the Wuhan

461
00:17:01,120 --> 00:17:04,000
laboratory and what they were doing and

462
00:17:02,320 --> 00:17:05,760
gain a function research, but I'm

463
00:17:04,000 --> 00:17:08,480
wondering if in, you know, a China or a

464
00:17:05,760 --> 00:17:10,720
Russia or an Iran or something, the

465
00:17:08,480 --> 00:17:12,079
government could fund a program for a

466
00:17:10,720 --> 00:17:14,319
small group of scientists to make a

467
00:17:12,079 --> 00:17:16,799
virus that they could, you know, I think

468
00:17:14,319 --> 00:17:18,400
they could. Yes. Now, they'd be worried

469
00:17:16,799 --> 00:17:19,600
about retaliation. They'd be worried

470
00:17:18,400 --> 00:17:21,120
about other governments doing the same

471
00:17:19,600 --> 00:17:22,959
to them. Hopefully, that would help keep

472
00:17:21,120 --> 00:17:24,559
it under control. They might also be

473
00:17:22,959 --> 00:17:28,079
worried about the virus spreading to

474
00:17:24,559 --> 00:17:31,520
their country. Okay? Then there's um

475
00:17:28,079 --> 00:17:33,679
corrupting elections.

476
00:17:31,520 --> 00:17:35,440
So, if you wanted to use AI to corrupt

477
00:17:33,679 --> 00:17:37,360
elections,

478
00:17:35,440 --> 00:17:40,160
a very effective thing is to be able to

479
00:17:37,360 --> 00:17:44,400
do targeted political advertisements

480
00:17:40,160 --> 00:17:47,200
where you know a lot about the person.

481
00:17:44,400 --> 00:17:49,760
So anybody who wanted to use AI for

482
00:17:47,200 --> 00:17:52,240
corrupting elections would try and get

483
00:17:49,760 --> 00:17:54,480
as much data as they could about

484
00:17:52,240 --> 00:17:56,720
everybody in the electorate. With that

485
00:17:54,480 --> 00:17:59,039
in mind, it's a bit worrying what Musk

486
00:17:56,720 --> 00:18:01,039
is doing at present in the States, going

487
00:17:59,039 --> 00:18:02,160
in and insisting on getting access to

488
00:18:01,039 --> 00:18:05,120
all these things that were very

489
00:18:02,160 --> 00:18:07,120
carefully siloed. The claim is it's to

490
00:18:05,120 --> 00:18:08,640
make things more efficient, but it's

491
00:18:07,120 --> 00:18:10,720
exactly what you would want if you

492
00:18:08,640 --> 00:18:11,840
intended to corrupt the next election.

493
00:18:10,720 --> 00:18:13,039
How do you mean? Because you get all

494
00:18:11,840 --> 00:18:14,960
this data on the people. You get all

495
00:18:13,039 --> 00:18:16,400
this data on people. You know how much

496
00:18:14,960 --> 00:18:18,720
they make where they you know everything

497
00:18:16,400 --> 00:18:21,280
about them. Once you know that, it's

498
00:18:18,720 --> 00:18:23,520
very easy to manipulate them because you

499
00:18:21,280 --> 00:18:25,919
can make an AI that you can send

500
00:18:23,520 --> 00:18:27,760
messages um that they'll find very

501
00:18:25,919 --> 00:18:29,600
convincing telling them not to vote, for

502
00:18:27,760 --> 00:18:32,880
example.

503
00:18:29,600 --> 00:18:35,200
So, I have no no reason other than

504
00:18:32,880 --> 00:18:36,559
common sense to think this, but I

505
00:18:35,200 --> 00:18:39,440
wouldn't be surprised if part of the

506
00:18:36,559 --> 00:18:42,559
motivation of getting all this data from

507
00:18:39,440 --> 00:18:45,360
American government sources is to

508
00:18:42,559 --> 00:18:47,200
corrupt elections. Another part might be

509
00:18:45,360 --> 00:18:49,679
that it's very nice training data for a

510
00:18:47,200 --> 00:18:51,280
big model, but he would have to be

511
00:18:49,679 --> 00:18:53,520
taking that data from the government and

512
00:18:51,280 --> 00:18:55,520
feeding it into his Yes. And what

513
00:18:53,520 --> 00:18:58,640
they've done is turned off lots of the

514
00:18:55,520 --> 00:19:00,400
security controls, got rid of the some

515
00:18:58,640 --> 00:19:02,799
of the organization to protect against

516
00:19:00,400 --> 00:19:06,720
that. Um, so that's corrupting

517
00:19:02,799 --> 00:19:09,200
elections. Okay. Then there's creating

518
00:19:06,720 --> 00:19:13,039
these two echo chambers

519
00:19:09,200 --> 00:19:16,640
by organizations like YouTube

520
00:19:13,039 --> 00:19:19,200
and Facebook showing people things that

521
00:19:16,640 --> 00:19:22,400
will make them indignant. People love to

522
00:19:19,200 --> 00:19:25,600
be indignant. Indignant as in angry or

523
00:19:22,400 --> 00:19:27,600
what does indignant mean? Feeling I'm

524
00:19:25,600 --> 00:19:30,640
sort of angry but feeling righteous.

525
00:19:27,600 --> 00:19:33,120
Okay. So, for example, if you were to

526
00:19:30,640 --> 00:19:34,799
show me something that said Trump did

527
00:19:33,120 --> 00:19:36,640
this crazy thing, here's a video of

528
00:19:34,799 --> 00:19:40,799
Trump doing this completely crazy thing.

529
00:19:36,640 --> 00:19:42,320
I would immediately click on it.

530
00:19:40,799 --> 00:19:45,919
Okay. So, putting us in echo chambers

531
00:19:42,320 --> 00:19:48,480
and dividing us. Yes. And that's um the

532
00:19:45,919 --> 00:19:51,520
policy that YouTube and Facebook and

533
00:19:48,480 --> 00:19:55,679
others use for deciding what to show you

534
00:19:51,520 --> 00:19:58,960
next is causing that. If they had a

535
00:19:55,679 --> 00:20:00,240
policy of showing you balanced things,

536
00:19:58,960 --> 00:20:01,280
they wouldn't get so many clicks and

537
00:20:00,240 --> 00:20:02,799
they wouldn't be able to sell so many

538
00:20:01,280 --> 00:20:04,400
advertisements.

539
00:20:02,799 --> 00:20:07,039
And so it's basically the profit motive

540
00:20:04,400 --> 00:20:09,039
is saying show them whatever will make

541
00:20:07,039 --> 00:20:11,280
them click. And what'll make them click

542
00:20:09,039 --> 00:20:13,600
is things that are more and more

543
00:20:11,280 --> 00:20:15,679
extreme. And that confirmed my existing

544
00:20:13,600 --> 00:20:17,280
bias. That confirm my existing bias. So

545
00:20:15,679 --> 00:20:19,440
you're getting your biases confirmed all

546
00:20:17,280 --> 00:20:20,720
the time further and further and further

547
00:20:19,440 --> 00:20:22,080
and further, which means you're you're

548
00:20:20,720 --> 00:20:23,679
driving away, which is now there's in

549
00:20:22,080 --> 00:20:25,360
the states there's two communities that

550
00:20:23,679 --> 00:20:26,640
don't hardly talk to each other. I'm not

551
00:20:25,360 --> 00:20:27,760
sure people realize that this is

552
00:20:26,640 --> 00:20:29,520
actually happening every time they open

553
00:20:27,760 --> 00:20:31,039
an app. But if you go on a Tik Tok or a

554
00:20:29,520 --> 00:20:33,280
YouTube or one of these big social

555
00:20:31,039 --> 00:20:35,440
networks, the algorithm, as you you

556
00:20:33,280 --> 00:20:38,080
said, is designed to show you more of

557
00:20:35,440 --> 00:20:39,840
the things that you had interest in last

558
00:20:38,080 --> 00:20:41,440
time. So, if you just play that out over

559
00:20:39,840 --> 00:20:42,960
10 years, it's going to drive you

560
00:20:41,440 --> 00:20:45,200
further and further and further into

561
00:20:42,960 --> 00:20:46,960
whatever ideology or belief you have and

562
00:20:45,200 --> 00:20:50,640
further away from nuance and common

563
00:20:46,960 --> 00:20:52,640
sense and um parity, which is a pretty

564
00:20:50,640 --> 00:20:53,919
remarkable thing. I I like people don't

565
00:20:52,640 --> 00:20:56,159
know it's happening. They just open

566
00:20:53,919 --> 00:20:59,039
their phones and experience something

567
00:20:56,159 --> 00:21:00,880
and think this is the news or the

568
00:20:59,039 --> 00:21:03,360
experience everyone else is having.

569
00:21:00,880 --> 00:21:04,880
Right. So, basically, if you have a

570
00:21:03,360 --> 00:21:07,039
newspaper and everybody gets the same

571
00:21:04,880 --> 00:21:08,720
newspaper, Yeah. you get to see all

572
00:21:07,039 --> 00:21:10,400
sorts of things you weren't looking for

573
00:21:08,720 --> 00:21:12,240
and you get a sense that if it's in the

574
00:21:10,400 --> 00:21:14,159
newspaper it's an important thing or

575
00:21:12,240 --> 00:21:18,159
significant thing but if you have your

576
00:21:14,159 --> 00:21:20,880
own news feed my news feed on my iPhone

577
00:21:18,159 --> 00:21:23,440
3/arters of the stories are about AI and

578
00:21:20,880 --> 00:21:25,440
I find it very hard to know if the whole

579
00:21:23,440 --> 00:21:28,960
world's talking about AI all the time or

580
00:21:25,440 --> 00:21:31,760
if it's just my newsfeed

581
00:21:28,960 --> 00:21:33,360
okay so driving me into my echo chambers

582
00:21:31,760 --> 00:21:34,720
um which is going to continue to divide

583
00:21:33,360 --> 00:21:36,480
us further and further I'm actually

584
00:21:34,720 --> 00:21:38,400
noticing that the algorithm are becoming

585
00:21:36,480 --> 00:21:40,559
even more,

586
00:21:38,400 --> 00:21:42,080
what's the word?

587
00:21:40,559 --> 00:21:43,520
Tailored. And people might go, "Oh,

588
00:21:42,080 --> 00:21:44,960
that's great." But what it means is

589
00:21:43,520 --> 00:21:47,440
they're becoming even more personalized,

590
00:21:44,960 --> 00:21:49,600
which is means that my reality is

591
00:21:47,440 --> 00:21:51,760
becoming even further from your reality.

592
00:21:49,600 --> 00:21:54,720
Yeah. It's crazy. We don't have a shared

593
00:21:51,760 --> 00:21:57,280
reality anymore. I share reality with

594
00:21:54,720 --> 00:21:59,120
other people who watch the BBC and other

595
00:21:57,280 --> 00:22:00,799
BBC news and other people who read the

596
00:21:59,120 --> 00:22:04,240
Guardian and other people who read the

597
00:22:00,799 --> 00:22:07,919
New York Times. I have almost no shared

598
00:22:04,240 --> 00:22:10,720
reality with people who watch Fox News.

599
00:22:07,919 --> 00:22:13,679
It's pretty It's pretty um I I It's

600
00:22:10,720 --> 00:22:15,280
worrisome. Yeah. Behind all this is the

601
00:22:13,679 --> 00:22:17,200
idea that these companies just want to

602
00:22:15,280 --> 00:22:19,440
make profit and they'll do whatever it

603
00:22:17,200 --> 00:22:22,000
takes to make more profit because they

604
00:22:19,440 --> 00:22:24,960
have to. They're legally obliged to do

605
00:22:22,000 --> 00:22:28,159
that. So, we almost can't blame the

606
00:22:24,960 --> 00:22:30,000
company, can we? If they're if Well,

607
00:22:28,159 --> 00:22:32,000
capitalism's done very well for us. It's

608
00:22:30,000 --> 00:22:34,480
produced lots of goodies. Yeah. But you

609
00:22:32,000 --> 00:22:37,840
need to have it very well regulated.

610
00:22:34,480 --> 00:22:40,640
So what you really want is to have rules

611
00:22:37,840 --> 00:22:43,200
so that when some company is trying to

612
00:22:40,640 --> 00:22:44,880
make as much profit as possible,

613
00:22:43,200 --> 00:22:47,200
in order to make that profit, they have

614
00:22:44,880 --> 00:22:48,960
to do things that are good for people in

615
00:22:47,200 --> 00:22:51,039
general, not things that are bad for

616
00:22:48,960 --> 00:22:52,400
people in general. So once you get to a

617
00:22:51,039 --> 00:22:54,480
situation where in order to make more

618
00:22:52,400 --> 00:22:57,039
profit the company starts doing things

619
00:22:54,480 --> 00:22:58,320
that are very bad for society like

620
00:22:57,039 --> 00:23:00,960
showing you things that are more and

621
00:22:58,320 --> 00:23:03,919
more extreme that's what regulations are

622
00:23:00,960 --> 00:23:06,000
for. So you need regulations with

623
00:23:03,919 --> 00:23:09,039
capitalism. Now companies will always

624
00:23:06,000 --> 00:23:11,039
say regulations get in the way make us

625
00:23:09,039 --> 00:23:12,799
less efficient and that's true. The

626
00:23:11,039 --> 00:23:14,400
whole point of regulations is to stop

627
00:23:12,799 --> 00:23:16,960
them doing things to make profit that

628
00:23:14,400 --> 00:23:18,640
hurt society. And we need strong

629
00:23:16,960 --> 00:23:20,559
regulation. who's going to decide

630
00:23:18,640 --> 00:23:23,520
whether it hurts society or not because

631
00:23:20,559 --> 00:23:25,120
you know that's the job of politicians

632
00:23:23,520 --> 00:23:26,400
unfortunately if the politicians are

633
00:23:25,120 --> 00:23:28,320
owned by the companies that's not so

634
00:23:26,400 --> 00:23:29,760
good and also the politicians might not

635
00:23:28,320 --> 00:23:31,039
understand the technology we you've

636
00:23:29,760 --> 00:23:32,480
probably seen the Senate hearings where

637
00:23:31,039 --> 00:23:34,559
they wheel out you know Mark Zuckerberg

638
00:23:32,480 --> 00:23:36,000
and these big tech CEOs and it is quite

639
00:23:34,559 --> 00:23:38,640
embarrassing because they're asking the

640
00:23:36,000 --> 00:23:42,080
wrong questions well I've seen the video

641
00:23:38,640 --> 00:23:44,400
of the US education secretary talking

642
00:23:42,080 --> 00:23:46,320
about how they're going to get AI in the

643
00:23:44,400 --> 00:23:48,559
classrooms except she thought it was

644
00:23:46,320 --> 00:23:50,080
called A1

645
00:23:48,559 --> 00:23:51,760
She's actually there saying we're going

646
00:23:50,080 --> 00:23:54,400
to have all the kids interacting with

647
00:23:51,760 --> 00:23:57,679
A1. There is a school system that's

648
00:23:54,400 --> 00:24:01,039
going to start um making sure that first

649
00:23:57,679 --> 00:24:02,880
graders or even preks have A1 teaching,

650
00:24:01,039 --> 00:24:04,480
you know, every year starting, you know,

651
00:24:02,880 --> 00:24:07,050
that far down in the grades. And that's

652
00:24:04,480 --> 00:24:10,880
just a that's a wonderful thing.

653
00:24:07,050 --> 00:24:12,080
[Laughter]

654
00:24:10,880 --> 00:24:14,559
And these are what these are the people

655
00:24:12,080 --> 00:24:16,080
that these are the people in charge.

656
00:24:14,559 --> 00:24:18,080
Ultimately the tech companies are in

657
00:24:16,080 --> 00:24:20,960
charge because they will outsmart the

658
00:24:18,080 --> 00:24:23,360
tech companies in the states now at

659
00:24:20,960 --> 00:24:26,320
least a few weeks ago when I was there

660
00:24:23,360 --> 00:24:28,320
they were running an advertisement about

661
00:24:26,320 --> 00:24:30,159
how it was very important not to

662
00:24:28,320 --> 00:24:32,320
regulate AI because it would hurt us in

663
00:24:30,159 --> 00:24:33,919
the competition with China. Yeah. And

664
00:24:32,320 --> 00:24:36,799
that's a that's a plausible argument

665
00:24:33,919 --> 00:24:38,880
there. Yes it will. But you have to

666
00:24:36,799 --> 00:24:44,000
decide, do you want to compete with

667
00:24:38,880 --> 00:24:46,480
China by doing things that will do a lot

668
00:24:44,000 --> 00:24:49,520
of harm to your society? And you

669
00:24:46,480 --> 00:24:51,440
probably don't.

670
00:24:49,520 --> 00:24:53,919
I guess they would say that it's not

671
00:24:51,440 --> 00:24:56,240
just China, it's Denmark and Australia

672
00:24:53,919 --> 00:24:57,919
and Canada and the UK. They're not so

673
00:24:56,240 --> 00:24:59,520
worried about and Germany. But if they

674
00:24:57,919 --> 00:25:01,520
kneecap themselves with regulation, if

675
00:24:59,520 --> 00:25:02,480
they slow themselves down, then the

676
00:25:01,520 --> 00:25:03,440
founders, the entrepreneurs, the

677
00:25:02,480 --> 00:25:05,919
investors are going to go. I think

678
00:25:03,440 --> 00:25:08,000
calling it kneecapping is taking a

679
00:25:05,919 --> 00:25:09,520
particular point of view is take taking

680
00:25:08,000 --> 00:25:11,679
the point of view that regulations are

681
00:25:09,520 --> 00:25:13,919
sort of very harmful. What you need to

682
00:25:11,679 --> 00:25:16,720
do is just constrain the big companies

683
00:25:13,919 --> 00:25:18,000
so that in order to make profit, they

684
00:25:16,720 --> 00:25:20,320
have to do things that are socially

685
00:25:18,000 --> 00:25:22,799
useful. Like Google search is a great

686
00:25:20,320 --> 00:25:24,880
example that didn't need regulation

687
00:25:22,799 --> 00:25:27,120
because it just made information

688
00:25:24,880 --> 00:25:29,600
available to people. It was great. But

689
00:25:27,120 --> 00:25:31,919
then if you take YouTube which starts

690
00:25:29,600 --> 00:25:33,919
showing you adverts and showing you more

691
00:25:31,919 --> 00:25:36,159
and more extreme things that needs

692
00:25:33,919 --> 00:25:38,880
regulation but we don't have the people

693
00:25:36,159 --> 00:25:41,760
to regulate it as we've identified. I

694
00:25:38,880 --> 00:25:43,360
think people know pretty well um that

695
00:25:41,760 --> 00:25:44,559
particular problem of showing you more

696
00:25:43,360 --> 00:25:46,400
and more extreme things. That's a

697
00:25:44,559 --> 00:25:49,120
well-known problem that the politicians

698
00:25:46,400 --> 00:25:51,760
understand. They just um need to get on

699
00:25:49,120 --> 00:25:53,200
and regulate it. So that was the the

700
00:25:51,760 --> 00:25:54,400
next point which was that the algorithms

701
00:25:53,200 --> 00:25:56,559
are going to drive us further into our

702
00:25:54,400 --> 00:26:00,159
echo chambers, right?

703
00:25:56,559 --> 00:26:03,679
What's next? Lethal autonomous weapons.

704
00:26:00,159 --> 00:26:05,840
Lethal autonomous weapons.

705
00:26:03,679 --> 00:26:07,279
That means things that can kill you and

706
00:26:05,840 --> 00:26:10,240
make their own decision about whether to

707
00:26:07,279 --> 00:26:12,240
kill you, which is the great dream, I

708
00:26:10,240 --> 00:26:14,480
guess, of the military-industrial

709
00:26:12,240 --> 00:26:17,120
complex being able to create such

710
00:26:14,480 --> 00:26:20,000
weapons. So, the worst thing about them

711
00:26:17,120 --> 00:26:23,440
is big powerful countries always have

712
00:26:20,000 --> 00:26:26,240
the ability to invade smaller poorer

713
00:26:23,440 --> 00:26:28,720
countries. they're just more powerful.

714
00:26:26,240 --> 00:26:31,360
But if you do that using actual

715
00:26:28,720 --> 00:26:34,880
soldiers, you get bodies coming back in

716
00:26:31,360 --> 00:26:37,120
bags and the relatives of the soldiers

717
00:26:34,880 --> 00:26:39,600
who were killed don't like it. So you

718
00:26:37,120 --> 00:26:42,080
get something like Vietnam. Mhm. In the

719
00:26:39,600 --> 00:26:44,320
end, there's a lot of protest at home.

720
00:26:42,080 --> 00:26:47,760
If instead of bodies coming back in

721
00:26:44,320 --> 00:26:49,679
bags, it was dead robots, there'd be

722
00:26:47,760 --> 00:26:51,279
much less protest and the

723
00:26:49,679 --> 00:26:53,120
military-industrial complex would like

724
00:26:51,279 --> 00:26:56,400
it much more because robots are

725
00:26:53,120 --> 00:26:58,880
expensive. And suppose you had something

726
00:26:56,400 --> 00:27:01,760
that could get killed and was expensive

727
00:26:58,880 --> 00:27:04,080
to replace. That would be just great.

728
00:27:01,760 --> 00:27:05,600
Big countries can invade small countries

729
00:27:04,080 --> 00:27:07,760
much more easily because they don't have

730
00:27:05,600 --> 00:27:12,720
their soldiers being killed. And the

731
00:27:07,760 --> 00:27:14,400
risk here is that these robots will

732
00:27:12,720 --> 00:27:16,640
malfunction or they'll just be more No,

733
00:27:14,400 --> 00:27:18,159
no, that's even if the robots do exactly

734
00:27:16,640 --> 00:27:20,559
what the people who built the robots

735
00:27:18,159 --> 00:27:22,400
want them to do, the risk is that it's

736
00:27:20,559 --> 00:27:23,679
going to make big countries invade small

737
00:27:22,400 --> 00:27:25,120
countries more often. More often because

738
00:27:23,679 --> 00:27:26,640
they can Yeah. And it's not a nice thing

739
00:27:25,120 --> 00:27:28,799
to do. So it brings down the friction of

740
00:27:26,640 --> 00:27:30,640
war. It brings down the cost of doing an

741
00:27:28,799 --> 00:27:32,960
invasion.

742
00:27:30,640 --> 00:27:34,640
And these machines will be smarter at

743
00:27:32,960 --> 00:27:36,880
warfare as well. So they'll be well even

744
00:27:34,640 --> 00:27:39,039
when the machines aren't smarter. So the

745
00:27:36,880 --> 00:27:42,559
lethal autonomous weapons, they can make

746
00:27:39,039 --> 00:27:44,640
them now. And they I think all the big

747
00:27:42,559 --> 00:27:46,240
defense models are busy making them.

748
00:27:44,640 --> 00:27:48,720
Even if they're not smarter than people,

749
00:27:46,240 --> 00:27:50,159
are still very nasty, scary things. Cuz

750
00:27:48,720 --> 00:27:53,279
I'm thinking that, you know, they could

751
00:27:50,159 --> 00:27:55,600
show just a picture. Go get this guy.

752
00:27:53,279 --> 00:27:58,960
Yeah. And go take out anyone he's been

753
00:27:55,600 --> 00:28:00,480
texting and this little wasp. So, two

754
00:27:58,960 --> 00:28:03,120
days ago, I was visiting a friend of

755
00:28:00,480 --> 00:28:05,440
mine in Sussex who had a drone that cost

756
00:28:03,120 --> 00:28:06,960
less than £200

757
00:28:05,440 --> 00:28:09,200
and

758
00:28:06,960 --> 00:28:11,600
the drone went up. It took a good look

759
00:28:09,200 --> 00:28:14,240
at me and then it could follow me

760
00:28:11,600 --> 00:28:15,840
through the woods and it follow It was

761
00:28:14,240 --> 00:28:17,760
very spooky having this drone. It was

762
00:28:15,840 --> 00:28:20,000
about 2 meters behind me. It was looking

763
00:28:17,760 --> 00:28:21,360
at me and if I moved over there, it

764
00:28:20,000 --> 00:28:24,399
moved over there. It could just track

765
00:28:21,360 --> 00:28:26,799
me. Mhm. For 200 pounds, but it was

766
00:28:24,399 --> 00:28:28,159
already quite spooky. Yeah. And I

767
00:28:26,799 --> 00:28:30,320
imagine there's as you say a race going

768
00:28:28,159 --> 00:28:33,679
on as we speak to who can build the most

769
00:28:30,320 --> 00:28:35,919
complex autonomous autonomous weapons.

770
00:28:33,679 --> 00:28:38,159
There is a a risk I often hear that some

771
00:28:35,919 --> 00:28:41,520
of these things will combine and the

772
00:28:38,159 --> 00:28:44,240
cyber attack will release weapons.

773
00:28:41,520 --> 00:28:46,240
Sure. Um you can you can get

774
00:28:44,240 --> 00:28:49,120
combinatorily many risks by combining

775
00:28:46,240 --> 00:28:51,039
these other risks. Mhm. So, I mean, for

776
00:28:49,120 --> 00:28:54,000
example, you could get a super

777
00:28:51,039 --> 00:28:56,080
intelligent AI that decides to get rid

778
00:28:54,000 --> 00:28:57,520
of people, and the obvious way to do

779
00:28:56,080 --> 00:29:01,679
that is just to make one of these nasty

780
00:28:57,520 --> 00:29:04,559
viruses. If you made a virus that was

781
00:29:01,679 --> 00:29:06,159
very contagious, very lethal, and very

782
00:29:04,559 --> 00:29:07,600
slow,

783
00:29:06,159 --> 00:29:09,600
everybody would have it before they

784
00:29:07,600 --> 00:29:11,120
realized what was happening. I mean, I

785
00:29:09,600 --> 00:29:13,600
think if a super intelligence wanted to

786
00:29:11,120 --> 00:29:14,880
get rid of us, it will probably go for

787
00:29:13,600 --> 00:29:16,480
something biological like that that

788
00:29:14,880 --> 00:29:17,919
wouldn't affect it. Do you not think it

789
00:29:16,480 --> 00:29:19,919
could just very quickly turn us against

790
00:29:17,919 --> 00:29:22,159
each other? For example, it could send a

791
00:29:19,919 --> 00:29:23,440
warning on the nuclear systems in

792
00:29:22,159 --> 00:29:26,320
America that there's a nuclear bomb

793
00:29:23,440 --> 00:29:28,640
coming from Russia or vice versa and one

794
00:29:26,320 --> 00:29:30,880
retaliates. Yeah. I mean, my basic view

795
00:29:28,640 --> 00:29:32,320
is there's so many ways in which the

796
00:29:30,880 --> 00:29:35,760
super intelligence could get rid of us.

797
00:29:32,320 --> 00:29:38,880
It's not worth speculating about.

798
00:29:35,760 --> 00:29:41,039
What What is What you have to do is

799
00:29:38,880 --> 00:29:43,200
prevent it ever wanting to. That's what

800
00:29:41,039 --> 00:29:45,440
we should be doing research on. There's

801
00:29:43,200 --> 00:29:46,960
no way we're going to prevent it from

802
00:29:45,440 --> 00:29:48,080
it's smarter than us, right? There's no

803
00:29:46,960 --> 00:29:50,880
way we're going to prevent it getting

804
00:29:48,080 --> 00:29:52,480
rid of us if it wants to. We're not used

805
00:29:50,880 --> 00:29:55,679
to thinking about things smarter than

806
00:29:52,480 --> 00:29:58,399
us. If you want to know what life's like

807
00:29:55,679 --> 00:30:01,880
when you're not the apex intelligence,

808
00:29:58,399 --> 00:30:01,880
ask a chicken.

809
00:30:03,200 --> 00:30:06,000
Yeah. I was thinking about my dog Pablo,

810
00:30:04,559 --> 00:30:08,399
my French bulldog, this morning as I

811
00:30:06,000 --> 00:30:10,880
left home. He has no idea where I'm

812
00:30:08,399 --> 00:30:13,200
going. He has no idea what I do, right?

813
00:30:10,880 --> 00:30:15,360
Can't even talk to him. Yeah. And the g

814
00:30:13,200 --> 00:30:17,279
the intelligence gap will be like that.

815
00:30:15,360 --> 00:30:19,520
So you're telling me that if I'm Pablo,

816
00:30:17,279 --> 00:30:24,480
my French bulldog, I need to figure out

817
00:30:19,520 --> 00:30:27,200
a way to make my owner not wipe me out.

818
00:30:24,480 --> 00:30:30,000
Yeah. So we have one example of that

819
00:30:27,200 --> 00:30:31,840
which is mothers and babies. Evolution

820
00:30:30,000 --> 00:30:33,520
put a lot of work into that. Mothers are

821
00:30:31,840 --> 00:30:35,440
smarter than babies, but babies are in

822
00:30:33,520 --> 00:30:37,440
control. And they're in control because

823
00:30:35,440 --> 00:30:39,840
the mother just can't bear lots of

824
00:30:37,440 --> 00:30:41,279
hormones and things, but the b the

825
00:30:39,840 --> 00:30:44,240
mother just can't bear the sound of the

826
00:30:41,279 --> 00:30:45,919
baby crying. Not all mothers. Not all

827
00:30:44,240 --> 00:30:48,399
mothers. And then the baby's not in

828
00:30:45,919 --> 00:30:51,760
control and then bad things happen. We

829
00:30:48,399 --> 00:30:53,840
somehow need to figure out how to make

830
00:30:51,760 --> 00:30:56,320
them not want to take over. The analogy

831
00:30:53,840 --> 00:30:57,760
I often use is forget about

832
00:30:56,320 --> 00:30:59,840
intelligence, think about physical

833
00:30:57,760 --> 00:31:02,159
strength. Suppose you have a nice little

834
00:30:59,840 --> 00:31:04,320
tiger cup. It's sort of bit bigger than

835
00:31:02,159 --> 00:31:06,480
a cat. It's really cute.

836
00:31:04,320 --> 00:31:08,720
It's very cuddly, very interesting to

837
00:31:06,480 --> 00:31:10,480
watch. Except that you better be sure

838
00:31:08,720 --> 00:31:12,159
that when it grows up, it never wants to

839
00:31:10,480 --> 00:31:15,279
kill you. Cuz if it ever wanted to kill

840
00:31:12,159 --> 00:31:16,720
you, you'd be dead in a few seconds. And

841
00:31:15,279 --> 00:31:19,760
you're saying the AI we have now is the

842
00:31:16,720 --> 00:31:21,360
target cub. Yep. And it's growing up.

843
00:31:19,760 --> 00:31:23,279
Yep.

844
00:31:21,360 --> 00:31:24,799
So, we need to train it as it's when

845
00:31:23,279 --> 00:31:26,480
it's a baby. Well, now a tiger has lots

846
00:31:24,799 --> 00:31:28,799
of in stuff built in. So, you know, when

847
00:31:26,480 --> 00:31:31,039
it grows up, it's not a safe thing to

848
00:31:28,799 --> 00:31:33,440
have around. But lions, people that have

849
00:31:31,039 --> 00:31:35,039
lions as pets, yes. Sometimes the lion

850
00:31:33,440 --> 00:31:37,279
is affectionate to its creator but not

851
00:31:35,039 --> 00:31:40,080
to others. Yes. And we don't know

852
00:31:37,279 --> 00:31:42,240
whether these AIs

853
00:31:40,080 --> 00:31:44,000
we we simply don't know whether we can

854
00:31:42,240 --> 00:31:46,000
make them not want to take over and not

855
00:31:44,000 --> 00:31:47,600
want to hurt us. Do you think we can? Do

856
00:31:46,000 --> 00:31:49,279
you think it's possible to train super

857
00:31:47,600 --> 00:31:51,679
intelligence? I don't think it's clear

858
00:31:49,279 --> 00:31:55,440
that we can. So I think it might be

859
00:31:51,679 --> 00:31:58,080
hopeless. But I also think we might be

860
00:31:55,440 --> 00:31:59,919
able to. And it'd be sort of crazy if

861
00:31:58,080 --> 00:32:02,320
people went extinct cuz we couldn't be

862
00:31:59,919 --> 00:32:04,559
bothered to try. If that's even a

863
00:32:02,320 --> 00:32:08,240
possibility, how do you feel about your

864
00:32:04,559 --> 00:32:09,760
life's work? Because you were Yeah. Um,

865
00:32:08,240 --> 00:32:12,159
it sort of takes the edge off it,

866
00:32:09,760 --> 00:32:13,840
doesn't it? I mean, the idea is going to

867
00:32:12,159 --> 00:32:16,480
be wonderful in healthcare and wonderful

868
00:32:13,840 --> 00:32:17,679
in education and wonderful. I mean, it's

869
00:32:16,480 --> 00:32:19,440
going to make call centers much more

870
00:32:17,679 --> 00:32:20,720
efficient, though one worries a bit

871
00:32:19,440 --> 00:32:24,080
about what the people who are doing that

872
00:32:20,720 --> 00:32:25,760
job now do. It makes me sad. I don't

873
00:32:24,080 --> 00:32:30,880
feel particularly guilty about

874
00:32:25,760 --> 00:32:32,960
developing AI like 40 years ago because

875
00:32:30,880 --> 00:32:34,799
at that time we had no idea that this

876
00:32:32,960 --> 00:32:36,399
stuff was going to happen this fast. We

877
00:32:34,799 --> 00:32:38,240
thought we had plenty of time to worry

878
00:32:36,399 --> 00:32:40,320
about things like that. They when you

879
00:32:38,240 --> 00:32:41,760
when you can't get the to do much, you

880
00:32:40,320 --> 00:32:44,399
want to get it to do a little bit more.

881
00:32:41,760 --> 00:32:46,320
You don't worry about this stupid little

882
00:32:44,399 --> 00:32:47,519
thing is going to take over from people.

883
00:32:46,320 --> 00:32:48,960
You just want it to be able to do a

884
00:32:47,519 --> 00:32:52,240
little bit more of the things people can

885
00:32:48,960 --> 00:32:54,960
do. It's not like I knowingly did

886
00:32:52,240 --> 00:32:56,720
something thinking this might wipe us

887
00:32:54,960 --> 00:33:00,240
all out, but I'm going to do it anyway.

888
00:32:56,720 --> 00:33:03,360
Mhm. But it is a bit sad that it's not

889
00:33:00,240 --> 00:33:05,200
just going to be something for good.

890
00:33:03,360 --> 00:33:07,039
So I feel I have a duty now to talk

891
00:33:05,200 --> 00:33:08,559
about the risks.

892
00:33:07,039 --> 00:33:10,159
And if you could play it forward and you

893
00:33:08,559 --> 00:33:11,440
could go forward 30, 50 years and you

894
00:33:10,159 --> 00:33:14,159
found out that it led to the extinction

895
00:33:11,440 --> 00:33:17,159
of humanity and if that does end up

896
00:33:14,159 --> 00:33:17,159
being

897
00:33:17,279 --> 00:33:20,760
being the outcome,

898
00:33:20,960 --> 00:33:25,679
well, if you played it forward and it

899
00:33:23,200 --> 00:33:28,960
led to the extinction of humanity, I

900
00:33:25,679 --> 00:33:30,960
would use that to tell people to tell

901
00:33:28,960 --> 00:33:32,399
their governments that we really have to

902
00:33:30,960 --> 00:33:34,880
work on how we're going to keep this

903
00:33:32,399 --> 00:33:36,399
stuff under control. I think we need

904
00:33:34,880 --> 00:33:38,720
people to tell governments that

905
00:33:36,399 --> 00:33:41,600
governments have to force the companies

906
00:33:38,720 --> 00:33:43,039
to use their resources to work on safety

907
00:33:41,600 --> 00:33:45,440
and they're not doing much of that

908
00:33:43,039 --> 00:33:46,960
because you don't make profits that way.

909
00:33:45,440 --> 00:33:51,440
One of your your students we talked

910
00:33:46,960 --> 00:33:53,840
about earlier um Ilia Yep. Ilia left

911
00:33:51,440 --> 00:33:55,360
OpenAI. Yep. And there was lots of

912
00:33:53,840 --> 00:33:57,760
conversation around the fact that he

913
00:33:55,360 --> 00:34:01,120
left because he had safety concerns.

914
00:33:57,760 --> 00:34:04,000
Yes. And he's gone on to set set up a AI

915
00:34:01,120 --> 00:34:06,480
safety company. Yes.

916
00:34:04,000 --> 00:34:08,000
Why do you think he left?

917
00:34:06,480 --> 00:34:11,200
I think he left because he had safety

918
00:34:08,000 --> 00:34:13,040
concerns. Really? He um I still have

919
00:34:11,200 --> 00:34:14,240
lunch with him from time to time. His

920
00:34:13,040 --> 00:34:16,240
parents live in Toronto. When he comes

921
00:34:14,240 --> 00:34:17,760
to Toronto, we have lunch together. He

922
00:34:16,240 --> 00:34:19,599
doesn't talk to me about what went on at

923
00:34:17,760 --> 00:34:22,320
Open AI, so I have no inside information

924
00:34:19,599 --> 00:34:24,639
about that. But I know I very well and

925
00:34:22,320 --> 00:34:26,560
he is genuinely concerned with safety.

926
00:34:24,639 --> 00:34:28,480
So I think that's why he left because he

927
00:34:26,560 --> 00:34:30,480
was one of the top people. I mean he was

928
00:34:28,480 --> 00:34:32,639
he was probably the most important

929
00:34:30,480 --> 00:34:35,440
person behind the development of um

930
00:34:32,639 --> 00:34:37,599
church GPT the the early versions like

931
00:34:35,440 --> 00:34:39,200
GPT2 he was very important in the

932
00:34:37,599 --> 00:34:41,919
development of that you know him

933
00:34:39,200 --> 00:34:43,839
personally so you know his character yes

934
00:34:41,919 --> 00:34:45,919
he has a good moral compass he's not

935
00:34:43,839 --> 00:34:48,560
like someone like Musco has no moral

936
00:34:45,919 --> 00:34:50,240
compass does Sam Alman have a good moral

937
00:34:48,560 --> 00:34:53,639
compass

938
00:34:50,240 --> 00:34:53,639
we'll see

939
00:34:53,760 --> 00:34:58,240
I don't know Sam so I don't want to

940
00:34:56,320 --> 00:35:00,880
comment on that. But from what you've

941
00:34:58,240 --> 00:35:02,880
seen, are you concerned about the

942
00:35:00,880 --> 00:35:04,400
actions that they've taken? Because if

943
00:35:02,880 --> 00:35:06,240
you know Ilia and Ilia's a good guy and

944
00:35:04,400 --> 00:35:08,640
he's left

945
00:35:06,240 --> 00:35:10,320
that would give you some insight. Yes.

946
00:35:08,640 --> 00:35:12,720
It would give you some reason to believe

947
00:35:10,320 --> 00:35:15,760
that there's a problem there. And if you

948
00:35:12,720 --> 00:35:17,839
look at Sam's statements

949
00:35:15,760 --> 00:35:20,880
some years ago,

950
00:35:17,839 --> 00:35:22,240
he sort of happily said in one interview

951
00:35:20,880 --> 00:35:23,760
and this stuff will probably kill us

952
00:35:22,240 --> 00:35:25,920
all. That's not exactly what he said,

953
00:35:23,760 --> 00:35:27,359
but that's what it amounted to. Now he's

954
00:35:25,920 --> 00:35:30,800
saying you don't need to worry too much

955
00:35:27,359 --> 00:35:32,960
about it. And I suspect that's not

956
00:35:30,800 --> 00:35:34,800
driven by

957
00:35:32,960 --> 00:35:38,160
seeking after the truth. That's driven

958
00:35:34,800 --> 00:35:40,800
by seeking after money. Is it money or

959
00:35:38,160 --> 00:35:42,800
is it power? Yeah. I shouldn't have said

960
00:35:40,800 --> 00:35:44,320
money. It's some some combination of

961
00:35:42,800 --> 00:35:46,960
those. Yes. Okay. I guess money is a

962
00:35:44,320 --> 00:35:50,079
proxy for power. But I am I've got a

963
00:35:46,960 --> 00:35:52,240
friend who's a billionaire and he is in

964
00:35:50,079 --> 00:35:54,320
those circles. And when I went to his

965
00:35:52,240 --> 00:35:55,839
house and had uh lunch with him one day,

966
00:35:54,320 --> 00:35:57,440
he knows lots of people in AI, building

967
00:35:55,839 --> 00:35:59,200
the biggest AI companies in the world.

968
00:35:57,440 --> 00:36:01,440
And he gave me a cautionary warning

969
00:35:59,200 --> 00:36:03,680
across the across his kitchen table in

970
00:36:01,440 --> 00:36:05,119
London where he gave me an insight into

971
00:36:03,680 --> 00:36:06,720
the private conversations these people

972
00:36:05,119 --> 00:36:08,560
have, not the media interviews they do

973
00:36:06,720 --> 00:36:09,839
where they talk about safety and all

974
00:36:08,560 --> 00:36:11,119
these things, but actually what some of

975
00:36:09,839 --> 00:36:13,200
these individuals think is going to

976
00:36:11,119 --> 00:36:15,359
happen and what do they think is going

977
00:36:13,200 --> 00:36:19,119
to happen. It's not what they say

978
00:36:15,359 --> 00:36:20,960
publicly. You know, one one person who I

979
00:36:19,119 --> 00:36:22,240
shouldn't name who is the who is leading

980
00:36:20,960 --> 00:36:24,079
one of the biggest AI companies in the

981
00:36:22,240 --> 00:36:25,839
world. He told me that he knows this

982
00:36:24,079 --> 00:36:27,200
person very well and he privately thinks

983
00:36:25,839 --> 00:36:29,760
that we're heading towards this kind of

984
00:36:27,200 --> 00:36:31,040
dystopian world where we have just huge

985
00:36:29,760 --> 00:36:33,760
amounts of free time. We don't work

986
00:36:31,040 --> 00:36:35,119
anymore. And this person doesn't really

987
00:36:33,760 --> 00:36:36,560
give a [ __ ] about the harm that it's

988
00:36:35,119 --> 00:36:37,920
going to have on the world. And this

989
00:36:36,560 --> 00:36:39,200
person who I'm referring to is building

990
00:36:37,920 --> 00:36:41,359
one of the biggest AI companies in the

991
00:36:39,200 --> 00:36:42,560
world. And I then watch this person's

992
00:36:41,359 --> 00:36:44,000
interviews online trying to figure out

993
00:36:42,560 --> 00:36:45,599
which of three people it is. Yeah. Well,

994
00:36:44,000 --> 00:36:46,880
it's one of those three people. Okay.

995
00:36:45,599 --> 00:36:48,480
And I watch this person's interviews

996
00:36:46,880 --> 00:36:50,320
online and I I reflect on a conversation

997
00:36:48,480 --> 00:36:52,320
that my billionaire friend had with me

998
00:36:50,320 --> 00:36:54,160
who knows him and I go, "Fucking hell,

999
00:36:52,320 --> 00:36:56,079
this guy's lying publicly." Like, he's

1000
00:36:54,160 --> 00:36:57,359
not telling the the truth to the world.

1001
00:36:56,079 --> 00:36:58,320
And that's haunted me a little bit. It's

1002
00:36:57,359 --> 00:36:59,920
part of the reason I have so many

1003
00:36:58,320 --> 00:37:00,880
conversations around AR in this podcast

1004
00:36:59,920 --> 00:37:04,160
because I'm like, I don't know if

1005
00:37:00,880 --> 00:37:06,720
they're I think they're a some of them

1006
00:37:04,160 --> 00:37:09,200
are a little bit sadistic about power. I

1007
00:37:06,720 --> 00:37:11,599
think they they like the idea that they

1008
00:37:09,200 --> 00:37:14,720
will change the world, that they will be

1009
00:37:11,599 --> 00:37:16,560
the one that fundamentally shifts the

1010
00:37:14,720 --> 00:37:19,440
world. I think Musk is clearly like

1011
00:37:16,560 --> 00:37:21,280
that, right?

1012
00:37:19,440 --> 00:37:22,480
He's such a complex character that I

1013
00:37:21,280 --> 00:37:24,720
don't I don't really know how to place

1014
00:37:22,480 --> 00:37:28,160
Musk. Um he's done some really good

1015
00:37:24,720 --> 00:37:29,599
things like um pushing electric cars.

1016
00:37:28,160 --> 00:37:31,359
That was a really good thing to do.

1017
00:37:29,599 --> 00:37:33,680
Yeah. Some of the things he said about

1018
00:37:31,359 --> 00:37:35,920
self-driving were a bit exaggerated, but

1019
00:37:33,680 --> 00:37:38,960
he that was a really useful thing he

1020
00:37:35,920 --> 00:37:41,920
did. Giving the Ukrainians communication

1021
00:37:38,960 --> 00:37:43,520
during the war with Russia. Stling. Um

1022
00:37:41,920 --> 00:37:45,359
that was a really good thing he did.

1023
00:37:43,520 --> 00:37:49,119
there's a bunch of things like that. Um,

1024
00:37:45,359 --> 00:37:53,720
but he's also done some very bad things.

1025
00:37:49,119 --> 00:37:53,720
So, coming back to this point of

1026
00:37:53,839 --> 00:38:01,520
the possibility of destruction

1027
00:37:57,839 --> 00:38:03,359
and the motives of these big companies,

1028
00:38:01,520 --> 00:38:05,440
are you at all hopeful that anything can

1029
00:38:03,359 --> 00:38:07,839
be done to slow down the pace and

1030
00:38:05,440 --> 00:38:10,800
acceleration of AI? Okay, there's two

1031
00:38:07,839 --> 00:38:12,720
issues. One is can you slow it down?

1032
00:38:10,800 --> 00:38:15,520
Yeah. And the other is, can you make it

1033
00:38:12,720 --> 00:38:18,160
so it will be safe in the end? It won't

1034
00:38:15,520 --> 00:38:20,480
wipe us all out. I don't believe we're

1035
00:38:18,160 --> 00:38:21,359
going to slow it down. Yeah. And the

1036
00:38:20,480 --> 00:38:22,800
reason I don't believe we're going to

1037
00:38:21,359 --> 00:38:24,880
slow it down is because there's

1038
00:38:22,800 --> 00:38:26,640
competition between countries and

1039
00:38:24,880 --> 00:38:28,880
competition between companies within a

1040
00:38:26,640 --> 00:38:31,599
country and all of that is making it go

1041
00:38:28,880 --> 00:38:34,480
faster and faster. And if the US slowed

1042
00:38:31,599 --> 00:38:38,240
it down, China wouldn't slow it down.

1043
00:38:34,480 --> 00:38:40,079
Does IA think it's possible to make AI

1044
00:38:38,240 --> 00:38:42,640
safe?

1045
00:38:40,079 --> 00:38:45,839
I think he does. He won't tell me what

1046
00:38:42,640 --> 00:38:47,119
his secret source is. I I'm not sure how

1047
00:38:45,839 --> 00:38:48,560
many people know what his secret source

1048
00:38:47,119 --> 00:38:50,240
is. I think a lot of the investors don't

1049
00:38:48,560 --> 00:38:51,599
know what his secret source is, but

1050
00:38:50,240 --> 00:38:53,200
they've given him billions of dollars

1051
00:38:51,599 --> 00:38:56,720
anyway because they have so much faith

1052
00:38:53,200 --> 00:38:59,280
in Asia, which isn't foolish. I mean, he

1053
00:38:56,720 --> 00:39:01,599
was very important in Alexet, which got

1054
00:38:59,280 --> 00:39:05,040
object recognition working well. He was

1055
00:39:01,599 --> 00:39:07,040
the main the main force behind the

1056
00:39:05,040 --> 00:39:10,480
things like GBC2

1057
00:39:07,040 --> 00:39:12,640
which then led to CH GPT.

1058
00:39:10,480 --> 00:39:14,720
So I think having a lot of faith in IA

1059
00:39:12,640 --> 00:39:16,400
is a very reasonable decision. There's

1060
00:39:14,720 --> 00:39:18,400
something quite haunting about the guy

1061
00:39:16,400 --> 00:39:20,880
that made and was the main force behind

1062
00:39:18,400 --> 00:39:24,240
GPT2 which led rise to this whole

1063
00:39:20,880 --> 00:39:26,560
revolution left the company because of

1064
00:39:24,240 --> 00:39:29,200
safety reasons. He knows something that

1065
00:39:26,560 --> 00:39:32,480
I don't know about what might happen

1066
00:39:29,200 --> 00:39:35,599
next. Well, the company had now I don't

1067
00:39:32,480 --> 00:39:37,200
know the precise details um but I'm

1068
00:39:35,599 --> 00:39:38,880
fairly sure the company had indicated

1069
00:39:37,200 --> 00:39:41,680
that would it would use a significant

1070
00:39:38,880 --> 00:39:44,320
fraction of its resources of the compute

1071
00:39:41,680 --> 00:39:46,560
time for doing safety research and then

1072
00:39:44,320 --> 00:39:47,680
it kept then it reduced that fraction. I

1073
00:39:46,560 --> 00:39:48,880
think that's one of the things that

1074
00:39:47,680 --> 00:39:51,760
happened. Yeah, that was reported

1075
00:39:48,880 --> 00:39:54,079
publicly. Yes. Yeah.

1076
00:39:51,760 --> 00:39:57,119
We've gotten to the autonomous weapons

1077
00:39:54,079 --> 00:40:00,560
part of the risk framework. Right. So

1078
00:39:57,119 --> 00:40:03,280
the next one is joblessness. Yeah. In

1079
00:40:00,560 --> 00:40:05,119
the past, new technologies have come in

1080
00:40:03,280 --> 00:40:07,599
which didn't lead to joblessness. New

1081
00:40:05,119 --> 00:40:09,680
jobs were created. So the classic

1082
00:40:07,599 --> 00:40:11,920
example people use is automatic tele

1083
00:40:09,680 --> 00:40:14,640
machines. When automatic tele machines

1084
00:40:11,920 --> 00:40:16,160
came in, a lot of bank tellers didn't

1085
00:40:14,640 --> 00:40:19,599
lose their jobs. They just got to do

1086
00:40:16,160 --> 00:40:21,920
more interesting things. But here, I

1087
00:40:19,599 --> 00:40:24,400
think this is more like when they got

1088
00:40:21,920 --> 00:40:26,640
machines in the industrial revolution.

1089
00:40:24,400 --> 00:40:28,720
And

1090
00:40:26,640 --> 00:40:30,560
you can't have a job digging ditches now

1091
00:40:28,720 --> 00:40:33,200
because a machine can dig ditches much

1092
00:40:30,560 --> 00:40:36,640
better than you can. And I think for

1093
00:40:33,200 --> 00:40:40,400
mundane intellectual labor, AI is just

1094
00:40:36,640 --> 00:40:43,359
going to replace everybody. Now, it will

1095
00:40:40,400 --> 00:40:46,000
may well be in the form of you have

1096
00:40:43,359 --> 00:40:48,240
fewer people using air assistance. So

1097
00:40:46,000 --> 00:40:50,800
it's a combination of a person and an AI

1098
00:40:48,240 --> 00:40:53,359
assistant are now doing the work that 10

1099
00:40:50,800 --> 00:40:55,040
people could do previously. People say

1100
00:40:53,359 --> 00:40:57,520
that it will create new jobs though, so

1101
00:40:55,040 --> 00:40:59,520
we'll be fine. Yes. And that's been the

1102
00:40:57,520 --> 00:41:01,680
case for other technologies, but this is

1103
00:40:59,520 --> 00:41:03,920
a very different kind of technology. If

1104
00:41:01,680 --> 00:41:05,440
it can do all mundane human intellectual

1105
00:41:03,920 --> 00:41:07,119
labor,

1106
00:41:05,440 --> 00:41:09,520
then what new jobs is it going to

1107
00:41:07,119 --> 00:41:11,200
create? You'd you'd have to be very

1108
00:41:09,520 --> 00:41:13,760
skilled to have a job that it couldn't

1109
00:41:11,200 --> 00:41:15,920
just do. So I don't I don't think

1110
00:41:13,760 --> 00:41:18,240
they're right. I think you can try and

1111
00:41:15,920 --> 00:41:20,640
generalize from other technologies that

1112
00:41:18,240 --> 00:41:22,480
have come in like computers or automatic

1113
00:41:20,640 --> 00:41:24,319
tele machines, but I think this is

1114
00:41:22,480 --> 00:41:26,319
different. People use this phrase. They

1115
00:41:24,319 --> 00:41:28,160
say AI won't take your job. A human

1116
00:41:26,319 --> 00:41:31,520
using AI will take your job. Yes, I

1117
00:41:28,160 --> 00:41:34,079
think that's true. But for many jobs,

1118
00:41:31,520 --> 00:41:36,720
that'll mean you need far fewer people.

1119
00:41:34,079 --> 00:41:39,440
My niece answers letters of complaint to

1120
00:41:36,720 --> 00:41:41,280
a health service. It used to take her 25

1121
00:41:39,440 --> 00:41:43,280
minutes. She'd read the complaint and

1122
00:41:41,280 --> 00:41:47,359
she'd think how to reply and she'd write

1123
00:41:43,280 --> 00:41:51,440
a letter. And now she just scans it into

1124
00:41:47,359 --> 00:41:53,040
um a chatbot and it writes the letter.

1125
00:41:51,440 --> 00:41:56,400
She just checks the letter. Occasionally

1126
00:41:53,040 --> 00:41:57,920
she tells it to revise it in some ways.

1127
00:41:56,400 --> 00:42:00,319
The whole process takes her five

1128
00:41:57,920 --> 00:42:03,119
minutes. That means she can answer five

1129
00:42:00,319 --> 00:42:06,640
times as many letters and that means

1130
00:42:03,119 --> 00:42:08,400
they need five times fewer of her so she

1131
00:42:06,640 --> 00:42:13,040
can do the job that five of her used to

1132
00:42:08,400 --> 00:42:15,119
do. Now, that will mean they need less

1133
00:42:13,040 --> 00:42:18,960
people. In other jobs, like in health

1134
00:42:15,119 --> 00:42:20,800
care, they're much more elastic. So, if

1135
00:42:18,960 --> 00:42:22,880
you could make doctors five times as

1136
00:42:20,800 --> 00:42:24,880
efficient, we could all have five times

1137
00:42:22,880 --> 00:42:27,280
as much health care for the same price,

1138
00:42:24,880 --> 00:42:28,880
and that would be great. There's there's

1139
00:42:27,280 --> 00:42:31,359
almost no limit to how much health care

1140
00:42:28,880 --> 00:42:34,319
people can absorb. They always want more

1141
00:42:31,359 --> 00:42:36,400
healthare if there's no cost to it.

1142
00:42:34,319 --> 00:42:38,560
There are jobs where you can make a

1143
00:42:36,400 --> 00:42:41,040
person with an AI assistant much more

1144
00:42:38,560 --> 00:42:42,800
efficient and you won't lead to less

1145
00:42:41,040 --> 00:42:46,000
people because you'll just have much

1146
00:42:42,800 --> 00:42:48,079
more of that being done. But most jobs I

1147
00:42:46,000 --> 00:42:49,119
think are not like that. Am I right in

1148
00:42:48,079 --> 00:42:50,720
thinking the sort of industrial

1149
00:42:49,119 --> 00:42:53,760
revolution

1150
00:42:50,720 --> 00:42:55,599
played a role in replacing muscles? Yes.

1151
00:42:53,760 --> 00:42:58,240
Exactly. And this revolution in AI

1152
00:42:55,599 --> 00:43:00,160
replaces intelligence the brain. Yeah.

1153
00:42:58,240 --> 00:43:03,440
So, so mundane intellectual labor is

1154
00:43:00,160 --> 00:43:05,920
like having strong muscles and it's not

1155
00:43:03,440 --> 00:43:07,760
worth much anymore. So, muscles have

1156
00:43:05,920 --> 00:43:11,839
been replaced. Now we intelligence is

1157
00:43:07,760 --> 00:43:13,440
being replaced. Yeah. So, what remains?

1158
00:43:11,839 --> 00:43:15,599
Maybe for a while some kinds of

1159
00:43:13,440 --> 00:43:18,000
creativity but the whole idea of super

1160
00:43:15,599 --> 00:43:19,280
intelligence is nothing remains. Um

1161
00:43:18,000 --> 00:43:21,040
these things will get to be better than

1162
00:43:19,280 --> 00:43:23,599
us at everything. So, what what do we

1163
00:43:21,040 --> 00:43:27,280
end up doing in such a world? Well, if

1164
00:43:23,599 --> 00:43:29,119
they work for us, we end up getting lots

1165
00:43:27,280 --> 00:43:32,880
of goods and services for not much

1166
00:43:29,119 --> 00:43:35,119
effort. Okay. But that sounds tempting

1167
00:43:32,880 --> 00:43:37,119
and nice, but I don't know. There's a

1168
00:43:35,119 --> 00:43:39,520
cautionary tale in creating more and

1169
00:43:37,119 --> 00:43:43,599
more ease for humans in in it going

1170
00:43:39,520 --> 00:43:46,240
badly. Yes. And we need to figure out if

1171
00:43:43,599 --> 00:43:50,480
we can make it go well. So the the nice

1172
00:43:46,240 --> 00:43:53,359
scenario is imagine a company with a CEO

1173
00:43:50,480 --> 00:43:56,400
who is very dumb, probably the son of

1174
00:43:53,359 --> 00:44:01,440
the former CEO. And he has an executive

1175
00:43:56,400 --> 00:44:03,680
assistant who's very smart and he says,

1176
00:44:01,440 --> 00:44:05,920
"I think we should do this." And the

1177
00:44:03,680 --> 00:44:07,680
executive assistant makes it all work.

1178
00:44:05,920 --> 00:44:09,520
The CEO feels great. He doesn't

1179
00:44:07,680 --> 00:44:12,240
understand that he's not really in

1180
00:44:09,520 --> 00:44:14,000
control. And in in some sense, he is in

1181
00:44:12,240 --> 00:44:16,000
control. He suggests what the company

1182
00:44:14,000 --> 00:44:18,079
should do. She just makes it all work.

1183
00:44:16,000 --> 00:44:21,040
Everything's great. That's the good

1184
00:44:18,079 --> 00:44:22,640
scenario. And the bad scenario, the bad

1185
00:44:21,040 --> 00:44:24,560
scenario, she thinks, "Why do we need

1186
00:44:22,640 --> 00:44:26,880
him?"

1187
00:44:24,560 --> 00:44:29,040
Yeah.

1188
00:44:26,880 --> 00:44:30,160
I mean, in a world where we have super

1189
00:44:29,040 --> 00:44:32,800
intelligence, which you don't believe is

1190
00:44:30,160 --> 00:44:34,480
that far away. Yeah, I think it might

1191
00:44:32,800 --> 00:44:36,400
not be that far away. It's very hard to

1192
00:44:34,480 --> 00:44:39,760
predict, but I think we might get it in

1193
00:44:36,400 --> 00:44:41,280
like 20 years or even less. I made the

1194
00:44:39,760 --> 00:44:43,920
biggest investment I've ever made in a

1195
00:44:41,280 --> 00:44:46,000
company because of my girlfriend. I came

1196
00:44:43,920 --> 00:44:48,079
home one night and my lovely girlfriend

1197
00:44:46,000 --> 00:44:49,839
was up at 1:00 a.m. in the morning

1198
00:44:48,079 --> 00:44:53,119
pulling her hair out as she tried to

1199
00:44:49,839 --> 00:44:55,200
piece together her own online store for

1200
00:44:53,119 --> 00:44:56,960
her business. And in that moment, I

1201
00:44:55,200 --> 00:44:59,920
remembered an email I'd had from a guy

1202
00:44:56,960 --> 00:45:02,079
called John, the founder of Stanto, our

1203
00:44:59,920 --> 00:45:04,000
new sponsor and a company I've invested

1204
00:45:02,079 --> 00:45:05,839
incredibly heavily in. And Standtore

1205
00:45:04,000 --> 00:45:07,760
helps creators to sell digital products,

1206
00:45:05,839 --> 00:45:10,079
courses, coaching, and memberships all

1207
00:45:07,760 --> 00:45:12,560
through a simple customizable link in

1208
00:45:10,079 --> 00:45:14,160
bio system. And it handles everything,

1209
00:45:12,560 --> 00:45:16,640
payments, bookings, emails, community

1210
00:45:14,160 --> 00:45:18,640
engagement, and even links with Shopify.

1211
00:45:16,640 --> 00:45:22,319
And I believe in it so much that I'm

1212
00:45:18,640 --> 00:45:23,839
going to launch a Stan challenge. And as

1213
00:45:22,319 --> 00:45:26,880
part of this challenge, I'm going to

1214
00:45:23,839 --> 00:45:28,319
give away $100,000 to one of you. If you

1215
00:45:26,880 --> 00:45:29,760
want to take part in this challenge, if

1216
00:45:28,319 --> 00:45:32,319
you want to monetize the knowledge that

1217
00:45:29,760 --> 00:45:34,880
you have, visit stephenbartlet.stan

1218
00:45:32,319 --> 00:45:37,200
stan.store to sign up. And you'll also

1219
00:45:34,880 --> 00:45:39,440
get an extended 30-day free trial of

1220
00:45:37,200 --> 00:45:41,440
Stan Store if you use that link. Your

1221
00:45:39,440 --> 00:45:43,040
next move could quite frankly change

1222
00:45:41,440 --> 00:45:45,440
everything. Because I talked about

1223
00:45:43,040 --> 00:45:47,920
ketosis on this podcast and ketones, a

1224
00:45:45,440 --> 00:45:49,520
brand called Ketone IQ sent me their

1225
00:45:47,920 --> 00:45:51,200
little product here and it was on my

1226
00:45:49,520 --> 00:45:52,560
desk when I got to the office. I picked

1227
00:45:51,200 --> 00:45:55,920
it up. It sat on my desk for a couple of

1228
00:45:52,560 --> 00:45:58,160
weeks. Then one day, I tried it and

1229
00:45:55,920 --> 00:46:01,119
honestly, I have not looked back ever

1230
00:45:58,160 --> 00:46:02,480
since. I now have this everywhere I go

1231
00:46:01,119 --> 00:46:03,920
when I travel all around the world. It's

1232
00:46:02,480 --> 00:46:05,280
in my hotel room. My team will put it

1233
00:46:03,920 --> 00:46:07,200
there. Before I did the podcast

1234
00:46:05,280 --> 00:46:09,839
recording today that I've just finished,

1235
00:46:07,200 --> 00:46:11,119
I had a shot of Ketone IQ. And as is

1236
00:46:09,839 --> 00:46:12,800
always the case when I fall in love with

1237
00:46:11,119 --> 00:46:14,640
a product, I called the CEO and asked if

1238
00:46:12,800 --> 00:46:16,560
I could invest a couple of million quid

1239
00:46:14,640 --> 00:46:18,000
into their company. So, I'm now an

1240
00:46:16,560 --> 00:46:20,640
investor in the company as well as them

1241
00:46:18,000 --> 00:46:22,960
being a brand sponsor. I find it so easy

1242
00:46:20,640 --> 00:46:24,319
to drop into deep focused work when I've

1243
00:46:22,960 --> 00:46:26,480
had one of these. I would love you to

1244
00:46:24,319 --> 00:46:28,079
try one and see the impact it has on

1245
00:46:26,480 --> 00:46:29,359
you, your focus, your productivity, and

1246
00:46:28,079 --> 00:46:31,599
your endurance. So, if you want to try

1247
00:46:29,359 --> 00:46:33,599
it today, visit ketone.com/stephven

1248
00:46:31,599 --> 00:46:35,359
for 30% off your subscription. Plus,

1249
00:46:33,599 --> 00:46:37,280
you'll receive a free gift with your

1250
00:46:35,359 --> 00:46:39,920
second shipment. That's

1251
00:46:37,280 --> 00:46:42,800
ketone.com/stephven.

1252
00:46:39,920 --> 00:46:43,920
I'm excited for you. I am. So, what's

1253
00:46:42,800 --> 00:46:45,280
the difference between what we have now

1254
00:46:43,920 --> 00:46:46,720
and super intelligence? Because it seems

1255
00:46:45,280 --> 00:46:50,800
to be really intelligent to me when I

1256
00:46:46,720 --> 00:46:52,960
use like chatbt3 or Gemini or Okay. So

1257
00:46:50,800 --> 00:46:55,200
it's already AI is already better than

1258
00:46:52,960 --> 00:46:59,040
us at a lot of things in particular

1259
00:46:55,200 --> 00:47:01,119
areas like chess for example. Yeah. AI

1260
00:46:59,040 --> 00:47:02,880
is so much better than us that people

1261
00:47:01,119 --> 00:47:05,599
will never beat those things again.

1262
00:47:02,880 --> 00:47:07,520
Maybe the occasional win but basically

1263
00:47:05,599 --> 00:47:10,000
they'll never be comparable again.

1264
00:47:07,520 --> 00:47:12,880
Obviously the same in go in terms of the

1265
00:47:10,000 --> 00:47:15,119
amount of knowledge they have. Um

1266
00:47:12,880 --> 00:47:17,440
something like GBT4 knows thousands of

1267
00:47:15,119 --> 00:47:19,280
times more than you do. There's a few

1268
00:47:17,440 --> 00:47:22,720
areas in which your knowledge is better

1269
00:47:19,280 --> 00:47:25,200
than its and in almost all areas it just

1270
00:47:22,720 --> 00:47:30,160
knows more than you do. What areas am I

1271
00:47:25,200 --> 00:47:33,040
better than it? Probably in interviewing

1272
00:47:30,160 --> 00:47:34,480
CEOs. You're probably better at that.

1273
00:47:33,040 --> 00:47:36,720
You've got a lot of experience at it.

1274
00:47:34,480 --> 00:47:39,440
You're a good interviewer. You know a

1275
00:47:36,720 --> 00:47:42,160
lot about it. If you tried if you got

1276
00:47:39,440 --> 00:47:46,079
GPT4 to interview a CEO, probably do a

1277
00:47:42,160 --> 00:47:47,680
worse job. Okay.

1278
00:47:46,079 --> 00:47:50,480
I'm trying to think if that if I agree

1279
00:47:47,680 --> 00:47:52,400
with that statement. Uh GPT4 I think for

1280
00:47:50,480 --> 00:47:54,160
sure. Yeah. Um but I but I guess you

1281
00:47:52,400 --> 00:47:55,359
could but it may not be long before

1282
00:47:54,160 --> 00:47:57,520
Yeah. I guess you could train one on

1283
00:47:55,359 --> 00:48:00,160
this how I ask questions and what I do

1284
00:47:57,520 --> 00:48:02,720
and Sure. And if you took a general

1285
00:48:00,160 --> 00:48:05,599
purpose sort of foundation model and

1286
00:48:02,720 --> 00:48:07,680
then you trained it up on not just you

1287
00:48:05,599 --> 00:48:10,319
but every every interviewer you could

1288
00:48:07,680 --> 00:48:12,000
find doing interviews like this but

1289
00:48:10,319 --> 00:48:13,520
especially you. You'll probably get to

1290
00:48:12,000 --> 00:48:17,119
be quite good at doing your job but

1291
00:48:13,520 --> 00:48:19,280
probably not as good as you for a while.

1292
00:48:17,119 --> 00:48:20,880
Okay. So, there's a few areas left and

1293
00:48:19,280 --> 00:48:23,599
then super intelligence becomes when

1294
00:48:20,880 --> 00:48:25,760
it's better than us at all things. When

1295
00:48:23,599 --> 00:48:27,359
it's much smarter than you and almost

1296
00:48:25,760 --> 00:48:29,040
all things is better than you. Yeah. And

1297
00:48:27,359 --> 00:48:32,319
you you you say that this might be a

1298
00:48:29,040 --> 00:48:34,079
decade away or so. Yeah. It might be. It

1299
00:48:32,319 --> 00:48:36,559
might be even closer. Some people think

1300
00:48:34,079 --> 00:48:38,240
it's even closer and might well be much

1301
00:48:36,559 --> 00:48:40,480
further. It might be 50 years away.

1302
00:48:38,240 --> 00:48:43,359
That's still a possibility. It might be

1303
00:48:40,480 --> 00:48:45,119
that somehow training on human data

1304
00:48:43,359 --> 00:48:47,760
limits you to not being much smarter

1305
00:48:45,119 --> 00:48:50,079
than humans. My guess is between 10 and

1306
00:48:47,760 --> 00:48:52,079
20 years we'll have super intelligence.

1307
00:48:50,079 --> 00:48:53,040
On this point of joblessness, it's

1308
00:48:52,079 --> 00:48:54,319
something that I've been thinking a lot

1309
00:48:53,040 --> 00:48:56,000
about in particular because I started

1310
00:48:54,319 --> 00:48:57,280
messing around with AI agents and we

1311
00:48:56,000 --> 00:48:58,960
released an episode on the podcast

1312
00:48:57,280 --> 00:49:01,280
actually this morning where we had a

1313
00:48:58,960 --> 00:49:03,200
debate about AI agents with some a CEO

1314
00:49:01,280 --> 00:49:05,359
of a big AI agent company and a few

1315
00:49:03,200 --> 00:49:08,160
other people and it was the first moment

1316
00:49:05,359 --> 00:49:10,079
where I had no it was another moment

1317
00:49:08,160 --> 00:49:11,760
where I had a Eureka moment about what

1318
00:49:10,079 --> 00:49:14,240
the future might look like when I was

1319
00:49:11,760 --> 00:49:16,319
able in the interview to tell this agent

1320
00:49:14,240 --> 00:49:17,599
to order all of us drinks and then 5

1321
00:49:16,319 --> 00:49:19,280
minutes later in the interview you see

1322
00:49:17,599 --> 00:49:21,119
the guy show up with the drinks and I

1323
00:49:19,280 --> 00:49:23,359
didn't touch anything. I just told it to

1324
00:49:21,119 --> 00:49:24,800
order us drinks to the studio. And you

1325
00:49:23,359 --> 00:49:26,000
didn't know about who you normally got

1326
00:49:24,800 --> 00:49:27,599
your drinks from. It figured that out

1327
00:49:26,000 --> 00:49:30,000
from the web. Yeah, figured out cuz it

1328
00:49:27,599 --> 00:49:32,000
went on Uber Eats. It has my my my data,

1329
00:49:30,000 --> 00:49:33,359
I guess. And it I we put it on the

1330
00:49:32,000 --> 00:49:34,880
screen in real time so everyone at home

1331
00:49:33,359 --> 00:49:36,800
could see the agent going through the

1332
00:49:34,880 --> 00:49:38,880
internet, picking the drinks, adding a

1333
00:49:36,800 --> 00:49:40,319
tip for the driver, putting my address

1334
00:49:38,880 --> 00:49:41,520
in, putting my credit card details in,

1335
00:49:40,319 --> 00:49:44,160
and then the next thing you see is the

1336
00:49:41,520 --> 00:49:46,000
drinks show up. So that was one moment.

1337
00:49:44,160 --> 00:49:49,280
And then the other moment was when I

1338
00:49:46,000 --> 00:49:50,800
used a tool called Replet and I built

1339
00:49:49,280 --> 00:49:53,920
software by just telling the agent what

1340
00:49:50,800 --> 00:49:56,720
I wanted. Yes. It's amazing, right? It's

1341
00:49:53,920 --> 00:49:58,559
amazing and terrifying at the same time.

1342
00:49:56,720 --> 00:50:00,400
Yes. Because and if it can build

1343
00:49:58,559 --> 00:50:03,440
software like that, right? Yeah.

1344
00:50:00,400 --> 00:50:06,720
Remember that the AI when it's training

1345
00:50:03,440 --> 00:50:08,480
is using code and if it can modify its

1346
00:50:06,720 --> 00:50:10,400
own code

1347
00:50:08,480 --> 00:50:12,160
then it gets quite scary, right? because

1348
00:50:10,400 --> 00:50:14,720
it can modify. It can change itself in a

1349
00:50:12,160 --> 00:50:17,920
way we can't change ourselves. We can't

1350
00:50:14,720 --> 00:50:19,359
change our innate endowment, right?

1351
00:50:17,920 --> 00:50:21,520
There's nothing about itself that it

1352
00:50:19,359 --> 00:50:23,040
couldn't change.

1353
00:50:21,520 --> 00:50:26,319
On this point of joblessness, you have

1354
00:50:23,040 --> 00:50:27,920
kids. I do. And they have kids. No, they

1355
00:50:26,319 --> 00:50:29,599
don't have kids. No grandkids yet. What

1356
00:50:27,920 --> 00:50:31,359
would you be saying to people about

1357
00:50:29,599 --> 00:50:33,040
their career prospects in a world of

1358
00:50:31,359 --> 00:50:35,680
super intelligence? What should we we be

1359
00:50:33,040 --> 00:50:37,599
thinking about? Um, in the meantime, I'd

1360
00:50:35,680 --> 00:50:40,319
say it's going to be a long time before

1361
00:50:37,599 --> 00:50:44,079
it's as good at physical manipulation as

1362
00:50:40,319 --> 00:50:47,400
us. Okay. And so, a good bet would be to

1363
00:50:44,079 --> 00:50:47,400
be a plumber.

1364
00:50:47,680 --> 00:50:51,599
until the humanoid robots show up in

1365
00:50:50,000 --> 00:50:53,200
such a world where there is mass

1366
00:50:51,599 --> 00:50:54,559
joblessness which is not something that

1367
00:50:53,200 --> 00:50:56,800
you just predict but this is something

1368
00:50:54,559 --> 00:50:59,440
that Sam Alman open AI I've heard him

1369
00:50:56,800 --> 00:51:00,720
predict and many of the CEOs Elon Musk I

1370
00:50:59,440 --> 00:51:02,559
watched an interview which I'll play on

1371
00:51:00,720 --> 00:51:04,000
screen of him being asked this question

1372
00:51:02,559 --> 00:51:05,599
and it's very rare that you see Elon

1373
00:51:04,000 --> 00:51:08,400
Musk silent for 12 seconds or whatever

1374
00:51:05,599 --> 00:51:10,319
it was and then he basically says

1375
00:51:08,400 --> 00:51:12,160
something about he actually is living in

1376
00:51:10,319 --> 00:51:14,160
suspended disbelief i.e. He's basically

1377
00:51:12,160 --> 00:51:15,760
just not thinking about it. When you

1378
00:51:14,160 --> 00:51:18,400
think about advising your children on a

1379
00:51:15,760 --> 00:51:19,920
career with so much that is changing,

1380
00:51:18,400 --> 00:51:22,920
what do you tell them is going to be of

1381
00:51:19,920 --> 00:51:22,920
value?

1382
00:51:33,280 --> 00:51:38,160
Well,

1383
00:51:35,680 --> 00:51:39,920
that is a tough question to answer. I

1384
00:51:38,160 --> 00:51:41,599
would just say, you know, to to sort of

1385
00:51:39,920 --> 00:51:43,599
follow their heart in terms of what they

1386
00:51:41,599 --> 00:51:45,680
they find um interesting to do or

1387
00:51:43,599 --> 00:51:48,720
fulfilling to do. I mean, if I think

1388
00:51:45,680 --> 00:51:53,119
about it too hard, frankly, it can be uh

1389
00:51:48,720 --> 00:51:56,480
dispariting and uh demotivating. Um

1390
00:51:53,119 --> 00:51:59,839
because I mean, I I go through I mean I

1391
00:51:56,480 --> 00:52:02,079
I I've put a lot of blood, sweat, and

1392
00:51:59,839 --> 00:52:04,240
tears into building the companies and

1393
00:52:02,079 --> 00:52:06,960
then it and then I'm like, wait, should

1394
00:52:04,240 --> 00:52:08,800
I be doing this? Because if I'm

1395
00:52:06,960 --> 00:52:11,599
sacrificing time with friends and family

1396
00:52:08,800 --> 00:52:13,760
that I would prefer to to to but but

1397
00:52:11,599 --> 00:52:16,720
then ultimately the AI can do all these

1398
00:52:13,760 --> 00:52:19,839
things. Does that make sense? I I don't

1399
00:52:16,720 --> 00:52:21,839
know. Um to some extent I have to have

1400
00:52:19,839 --> 00:52:26,079
deliberate suspension of disbelief in

1401
00:52:21,839 --> 00:52:30,599
order to to remain motivated. Um so I I

1402
00:52:26,079 --> 00:52:30,599
guess I would say just you know

1403
00:52:31,119 --> 00:52:34,559
work on things that you find

1404
00:52:32,240 --> 00:52:36,559
interesting, fulfilling and um and and

1405
00:52:34,559 --> 00:52:39,359
that contribute uh some good to the rest

1406
00:52:36,559 --> 00:52:42,880
of society. Yeah. A lot of these threats

1407
00:52:39,359 --> 00:52:45,200
it's very hard to intellectually you can

1408
00:52:42,880 --> 00:52:47,839
see the threat but it's very hard to

1409
00:52:45,200 --> 00:52:49,520
come to terms with it emotionally.

1410
00:52:47,839 --> 00:52:50,800
Yeah. I haven't come to terms with it

1411
00:52:49,520 --> 00:52:53,280
emotionally yet. What do you mean by

1412
00:52:50,800 --> 00:52:56,880
that?

1413
00:52:53,280 --> 00:52:58,720
I haven't come to terms with what the

1414
00:52:56,880 --> 00:53:01,920
development of super intelligence could

1415
00:52:58,720 --> 00:53:04,319
do to my children's future.

1416
00:53:01,920 --> 00:53:07,119
I'm okay. I'm 77.

1417
00:53:04,319 --> 00:53:09,920
I'm going to be out of here soon. But

1418
00:53:07,119 --> 00:53:13,680
for my children and my my younger

1419
00:53:09,920 --> 00:53:17,839
friends, my nephews and nieces and their

1420
00:53:13,680 --> 00:53:19,520
children, um

1421
00:53:17,839 --> 00:53:22,680
I just don't like to think about what

1422
00:53:19,520 --> 00:53:22,680
could happen.

1423
00:53:23,680 --> 00:53:28,760
Why? Cuz it could be awful.

1424
00:53:29,839 --> 00:53:35,680
In In what way?

1425
00:53:32,480 --> 00:53:37,440
Well, if I ever decided to take over. I

1426
00:53:35,680 --> 00:53:40,400
mean, it would need people for a while

1427
00:53:37,440 --> 00:53:42,079
to run the power stations until it

1428
00:53:40,400 --> 00:53:45,359
designed better analog machines to run

1429
00:53:42,079 --> 00:53:47,760
the power stations. There's so many ways

1430
00:53:45,359 --> 00:53:50,480
it could get rid of people, all of which

1431
00:53:47,760 --> 00:53:51,920
would of course be very nasty.

1432
00:53:50,480 --> 00:53:54,800
Is that part of the reason you do what

1433
00:53:51,920 --> 00:53:57,839
you do now? Yeah. I I mean, I think we

1434
00:53:54,800 --> 00:53:59,520
should be making a huge effort right now

1435
00:53:57,839 --> 00:54:01,920
to try and figure out if we can develop

1436
00:53:59,520 --> 00:54:03,680
it safely. Are you concerned about the

1437
00:54:01,920 --> 00:54:05,520
midterm impact potentially on your

1438
00:54:03,680 --> 00:54:07,440
nephews and your your kids in terms of

1439
00:54:05,520 --> 00:54:09,280
their jobs as well? Yeah, I'm concerned

1440
00:54:07,440 --> 00:54:10,640
about all that. Are there any particular

1441
00:54:09,280 --> 00:54:11,920
industries that you think are most at

1442
00:54:10,640 --> 00:54:14,160
risk? People talk about the creative

1443
00:54:11,920 --> 00:54:16,240
industries a lot and sort of knowledge

1444
00:54:14,160 --> 00:54:17,760
work. They talk about lawyers and

1445
00:54:16,240 --> 00:54:19,119
accountants and stuff like that. Yeah.

1446
00:54:17,760 --> 00:54:21,040
So, that's why I mentioned plumbers. I

1447
00:54:19,119 --> 00:54:22,319
think plumbers are less at risk. Okay,

1448
00:54:21,040 --> 00:54:26,319
I'm going to become a plumber. Someone

1449
00:54:22,319 --> 00:54:28,400
like a legal assistant, a parallegal.

1450
00:54:26,319 --> 00:54:29,920
Um they're not going to be needed for

1451
00:54:28,400 --> 00:54:32,160
very long. And is there a wealth

1452
00:54:29,920 --> 00:54:34,319
inequality issue here that will will

1453
00:54:32,160 --> 00:54:37,839
arise from this? Yeah, I think in a

1454
00:54:34,319 --> 00:54:39,280
society which shared out things fairly,

1455
00:54:37,839 --> 00:54:41,760
if you get a big increase in

1456
00:54:39,280 --> 00:54:43,520
productivity, everybody should be better

1457
00:54:41,760 --> 00:54:46,640
off.

1458
00:54:43,520 --> 00:54:48,640
But if you can replace lots of people by

1459
00:54:46,640 --> 00:54:50,880
AIS,

1460
00:54:48,640 --> 00:54:52,720
then the people who get replaced will be

1461
00:54:50,880 --> 00:54:55,760
worse off

1462
00:54:52,720 --> 00:54:58,880
and the company that supplies the AIS

1463
00:54:55,760 --> 00:55:01,599
will be much better off

1464
00:54:58,880 --> 00:55:03,280
and the company that uses the AIS. So

1465
00:55:01,599 --> 00:55:06,079
it's going to increase the gap between

1466
00:55:03,280 --> 00:55:08,319
rich and poor. And we know that if you

1467
00:55:06,079 --> 00:55:09,760
look at that gap between rich and poor,

1468
00:55:08,319 --> 00:55:12,400
that basically tells you how nice the

1469
00:55:09,760 --> 00:55:14,240
society is. If you have a big gap, you

1470
00:55:12,400 --> 00:55:17,359
get very nasty societies in which people

1471
00:55:14,240 --> 00:55:21,040
live in world communities and put other

1472
00:55:17,359 --> 00:55:22,800
people in mass jails. It's not good to

1473
00:55:21,040 --> 00:55:24,480
increase the gap between rich and poor.

1474
00:55:22,800 --> 00:55:25,839
The International Monetary Fund has

1475
00:55:24,480 --> 00:55:28,400
expressed profound concerns that

1476
00:55:25,839 --> 00:55:30,559
generative AI could cause massive labor

1477
00:55:28,400 --> 00:55:32,160
disruptions and rising inequality and

1478
00:55:30,559 --> 00:55:34,240
has called for policies that prevent

1479
00:55:32,160 --> 00:55:36,640
this from happening. I read that in the

1480
00:55:34,240 --> 00:55:38,319
business insider. So, have they given

1481
00:55:36,640 --> 00:55:40,720
any of what the policies should look

1482
00:55:38,319 --> 00:55:42,079
like? No. Yeah, that's the problem. I

1483
00:55:40,720 --> 00:55:44,000
mean, if AI can make everything much

1484
00:55:42,079 --> 00:55:47,599
more efficient and get rid of people for

1485
00:55:44,000 --> 00:55:51,680
most jobs or have a person assisted by I

1486
00:55:47,599 --> 00:55:53,440
doing many many people's work, it's not

1487
00:55:51,680 --> 00:55:55,760
obvious what to do about it. It's

1488
00:55:53,440 --> 00:55:58,000
universal basic income,

1489
00:55:55,760 --> 00:56:01,839
give everybody money. Yeah, I I I think

1490
00:55:58,000 --> 00:56:04,400
that's a good start and it stops people

1491
00:56:01,839 --> 00:56:06,319
starving. But for a lot of people, their

1492
00:56:04,400 --> 00:56:08,480
dignity is tied up with their job. I

1493
00:56:06,319 --> 00:56:11,040
mean, who you think you are is tied up

1494
00:56:08,480 --> 00:56:13,599
with you doing this job, right? Yeah.

1495
00:56:11,040 --> 00:56:16,400
And if we said, "We'll give you the same

1496
00:56:13,599 --> 00:56:18,880
money just to sit around," that would

1497
00:56:16,400 --> 00:56:21,680
impact your dignity. You said something

1498
00:56:18,880 --> 00:56:23,599
earlier about it surpassing or being

1499
00:56:21,680 --> 00:56:26,559
superior to human intelligence. A lot of

1500
00:56:23,599 --> 00:56:28,079
people, I think, like to believe that AI

1501
00:56:26,559 --> 00:56:29,839
is is on a computer and it's something

1502
00:56:28,079 --> 00:56:31,760
you can just turn off if you don't like

1503
00:56:29,839 --> 00:56:35,760
it. Well, let me tell you why I think

1504
00:56:31,760 --> 00:56:38,880
it's superior. Okay. Um, it's digital.

1505
00:56:35,760 --> 00:56:40,720
And because it's digital, you can have

1506
00:56:38,880 --> 00:56:42,720
you can simulate a neural network on one

1507
00:56:40,720 --> 00:56:44,799
piece of hardware. Yeah. And you can

1508
00:56:42,720 --> 00:56:47,119
simulate exactly the same neural network

1509
00:56:44,799 --> 00:56:48,400
on a different piece of hardware. So you

1510
00:56:47,119 --> 00:56:50,160
can have clones of the same

1511
00:56:48,400 --> 00:56:52,880
intelligence.

1512
00:56:50,160 --> 00:56:54,880
Now you could get this one to go off and

1513
00:56:52,880 --> 00:56:56,559
look at one bit of the internet and this

1514
00:56:54,880 --> 00:56:58,319
other one to look at a different bit of

1515
00:56:56,559 --> 00:57:00,480
the internet. And while they're looking

1516
00:56:58,319 --> 00:57:02,559
at these different bits of the internet,

1517
00:57:00,480 --> 00:57:04,319
they can be syncing with each other. So

1518
00:57:02,559 --> 00:57:05,680
they keep their weights the same, the

1519
00:57:04,319 --> 00:57:07,520
connection strengths the same. Weights

1520
00:57:05,680 --> 00:57:08,720
are connection strengths. Mhm. So this

1521
00:57:07,520 --> 00:57:10,160
one might look at something on the

1522
00:57:08,720 --> 00:57:11,280
internet and say, "Oh, I'd like to

1523
00:57:10,160 --> 00:57:14,160
increase this strength of this

1524
00:57:11,280 --> 00:57:16,400
connection a bit." And it can convey

1525
00:57:14,160 --> 00:57:17,520
that information to this one. So it can

1526
00:57:16,400 --> 00:57:19,520
increase the strength of that connection

1527
00:57:17,520 --> 00:57:20,880
a bit based on this one's experience.

1528
00:57:19,520 --> 00:57:22,160
And when you say the strength of the

1529
00:57:20,880 --> 00:57:24,319
connection, you're talking about

1530
00:57:22,160 --> 00:57:26,799
learning. That's learning. Yes. Learning

1531
00:57:24,319 --> 00:57:28,480
consists of saying instead of this one

1532
00:57:26,799 --> 00:57:30,240
giving 2.4 four votes for whether that

1533
00:57:28,480 --> 00:57:31,920
one should turn on. We'll have this one

1534
00:57:30,240 --> 00:57:33,839
give 2.5 votes for whether this one

1535
00:57:31,920 --> 00:57:36,000
should turn on. And that will be a

1536
00:57:33,839 --> 00:57:39,440
little bit of learning. So these two

1537
00:57:36,000 --> 00:57:41,280
different copies of the same neural net

1538
00:57:39,440 --> 00:57:43,200
are getting different experiences.

1539
00:57:41,280 --> 00:57:44,720
They're looking at different data, but

1540
00:57:43,200 --> 00:57:47,280
they're sharing what they've learned by

1541
00:57:44,720 --> 00:57:49,680
averaging their weights together. Mhm.

1542
00:57:47,280 --> 00:57:52,160
And they can do that averaging at like a

1543
00:57:49,680 --> 00:57:54,319
you can average a trillion weights. When

1544
00:57:52,160 --> 00:57:55,920
you and I transfer information, we're

1545
00:57:54,319 --> 00:57:57,440
limited to the amount of information in

1546
00:57:55,920 --> 00:57:59,280
a sentence. And the amount of

1547
00:57:57,440 --> 00:58:01,839
information in a sentence is maybe a 100

1548
00:57:59,280 --> 00:58:03,200
bits. It's very little information.

1549
00:58:01,839 --> 00:58:05,440
We're lucky if we're transferring like

1550
00:58:03,200 --> 00:58:08,079
10 bits a second. These things are

1551
00:58:05,440 --> 00:58:09,680
transferring trillions of bits a second.

1552
00:58:08,079 --> 00:58:12,240
So, they're billions of times better

1553
00:58:09,680 --> 00:58:14,480
than us at sharing information.

1554
00:58:12,240 --> 00:58:16,240
And that's because they're digital. And

1555
00:58:14,480 --> 00:58:18,160
you can have two bits of hardware using

1556
00:58:16,240 --> 00:58:20,079
the connection strengths in exactly the

1557
00:58:18,160 --> 00:58:21,520
same way. We're analog and you can't do

1558
00:58:20,079 --> 00:58:24,480
that. Your brain's different from my

1559
00:58:21,520 --> 00:58:26,400
brain. And if I could see the connection

1560
00:58:24,480 --> 00:58:27,760
strengths between all your neurons, it

1561
00:58:26,400 --> 00:58:29,200
wouldn't do me any good because my

1562
00:58:27,760 --> 00:58:30,240
neurons work slightly differently and

1563
00:58:29,200 --> 00:58:33,280
they're connected up slightly

1564
00:58:30,240 --> 00:58:35,920
differently. Mhm. So when you die, all

1565
00:58:33,280 --> 00:58:38,000
your knowledge dies with you. When these

1566
00:58:35,920 --> 00:58:39,920
things die, suppose you take these two

1567
00:58:38,000 --> 00:58:42,079
digital intelligences that are clones of

1568
00:58:39,920 --> 00:58:44,400
each other and you destroy the hardware

1569
00:58:42,079 --> 00:58:46,160
they run on. As long as you've stored

1570
00:58:44,400 --> 00:58:48,880
the connection strength somewhere, you

1571
00:58:46,160 --> 00:58:51,040
can just build new hardware that

1572
00:58:48,880 --> 00:58:52,640
executes the same instructions. So,

1573
00:58:51,040 --> 00:58:54,480
it'll know how to use those connection

1574
00:58:52,640 --> 00:58:56,880
strengths and you've recreated that

1575
00:58:54,480 --> 00:58:58,400
intelligence. So, they're immortal.

1576
00:58:56,880 --> 00:59:01,119
We've actually solved the problem of

1577
00:58:58,400 --> 00:59:05,200
immortality, but it's only for digital

1578
00:59:01,119 --> 00:59:06,960
things. So, it knows it will essentially

1579
00:59:05,200 --> 00:59:09,280
know everything that humans know but

1580
00:59:06,960 --> 00:59:11,599
more because it will learn new things.

1581
00:59:09,280 --> 00:59:13,440
It will learn new things. It would also

1582
00:59:11,599 --> 00:59:15,760
see all sorts of analogies that people

1583
00:59:13,440 --> 00:59:18,960
probably never saw.

1584
00:59:15,760 --> 00:59:21,920
So, for example, at the point when GPT4

1585
00:59:18,960 --> 00:59:24,079
couldn't look on the web, I asked it,

1586
00:59:21,920 --> 00:59:26,000
"Why is a compost heap like an atom

1587
00:59:24,079 --> 00:59:28,960
bomb?"

1588
00:59:26,000 --> 00:59:30,880
Off you go. I have no idea. Exactly.

1589
00:59:28,960 --> 00:59:32,880
Excellent. Most that's exactly what most

1590
00:59:30,880 --> 00:59:35,119
people would say. It said, "Well, the

1591
00:59:32,880 --> 00:59:37,200
time scales are very different and the

1592
00:59:35,119 --> 00:59:38,720
energy scales are very different." But

1593
00:59:37,200 --> 00:59:40,960
then I went on to talk about how a

1594
00:59:38,720 --> 00:59:44,160
compost he as it gets hotter generates

1595
00:59:40,960 --> 00:59:46,079
heat faster and an atom bomb as it

1596
00:59:44,160 --> 00:59:48,640
produces more neutrons generates

1597
00:59:46,079 --> 00:59:50,720
neutrons faster. And so they're both

1598
00:59:48,640 --> 00:59:52,960
chain reactions but at very different

1599
00:59:50,720 --> 00:59:56,799
time in energy scales. And I believe

1600
00:59:52,960 --> 00:59:58,640
GPT4 had seen that during its training.

1601
00:59:56,799 --> 01:00:00,240
It had understood the analogy between a

1602
00:59:58,640 --> 01:00:02,400
compost heap and an atom bomb. And the

1603
01:00:00,240 --> 01:00:04,319
reason I believe that is if you've only

1604
01:00:02,400 --> 01:00:06,880
got a trillion connections, remember you

1605
01:00:04,319 --> 01:00:08,559
have 100 trillion. And you need to have

1606
01:00:06,880 --> 01:00:11,680
thousands of times more knowledge than a

1607
01:00:08,559 --> 01:00:14,240
person, you need to compress information

1608
01:00:11,680 --> 01:00:16,400
into those connections. And to compress

1609
01:00:14,240 --> 01:00:18,240
information, you need to see analogies

1610
01:00:16,400 --> 01:00:20,160
between different things. In other

1611
01:00:18,240 --> 01:00:21,680
words, it needs to see all the things

1612
01:00:20,160 --> 01:00:23,359
that are chain reactions and understand

1613
01:00:21,680 --> 01:00:25,680
the basic idea of a chain reaction and

1614
01:00:23,359 --> 01:00:26,960
code that code the ways in which they're

1615
01:00:25,680 --> 01:00:28,799
different. And that's just a more

1616
01:00:26,960 --> 01:00:31,280
efficient way of coding things than

1617
01:00:28,799 --> 01:00:33,920
coding each of them separately.

1618
01:00:31,280 --> 01:00:35,599
So it's seen many many analogies

1619
01:00:33,920 --> 01:00:38,000
probably many analogies that people have

1620
01:00:35,599 --> 01:00:39,200
never seen. That's why I also think that

1621
01:00:38,000 --> 01:00:40,799
people who say these things will never

1622
01:00:39,200 --> 01:00:43,119
be creative. They're going to be much

1623
01:00:40,799 --> 01:00:44,559
more creative than us because they're

1624
01:00:43,119 --> 01:00:46,720
going to see all sorts of analogies we

1625
01:00:44,559 --> 01:00:49,119
never saw. And a lot of creativity is

1626
01:00:46,720 --> 01:00:50,720
about seeing strange analogies.

1627
01:00:49,119 --> 01:00:52,640
People are somewhat romantic about the

1628
01:00:50,720 --> 01:00:54,160
specialness of what it is to be human.

1629
01:00:52,640 --> 01:00:55,440
And you hear lots of people saying it's

1630
01:00:54,160 --> 01:00:57,200
very very different. It's a it's a

1631
01:00:55,440 --> 01:00:59,920
computer. We are, you know, we're

1632
01:00:57,200 --> 01:01:02,319
conscious. We are creatives. We we have

1633
01:00:59,920 --> 01:01:04,400
these sort of innate unique abilities

1634
01:01:02,319 --> 01:01:06,400
that the computers will never have. What

1635
01:01:04,400 --> 01:01:09,680
do you say to those people? I'd argue a

1636
01:01:06,400 --> 01:01:11,760
bit with the innate. Um,

1637
01:01:09,680 --> 01:01:13,599
so

1638
01:01:11,760 --> 01:01:15,440
the first thing I say is we have a long

1639
01:01:13,599 --> 01:01:17,680
history of believing people were

1640
01:01:15,440 --> 01:01:19,680
special. And we should have learned by

1641
01:01:17,680 --> 01:01:21,760
now. We thought we were at the center of

1642
01:01:19,680 --> 01:01:24,000
the universe. We thought we were made in

1643
01:01:21,760 --> 01:01:27,119
the image of God. white people thought

1644
01:01:24,000 --> 01:01:29,440
they were very special. We just tend to

1645
01:01:27,119 --> 01:01:33,680
want to think we're special.

1646
01:01:29,440 --> 01:01:35,599
My belief is that more or less everyone

1647
01:01:33,680 --> 01:01:37,839
has a completely wrong model of what the

1648
01:01:35,599 --> 01:01:41,920
mind is. Let's suppose I drink a lot or

1649
01:01:37,839 --> 01:01:43,839
I drop some acid and not recommended and

1650
01:01:41,920 --> 01:01:46,000
I

1651
01:01:43,839 --> 01:01:47,359
say to you I have the subjective

1652
01:01:46,000 --> 01:01:50,000
experience of little pink elephants

1653
01:01:47,359 --> 01:01:51,839
floating in front of me. Mhm. Most

1654
01:01:50,000 --> 01:01:54,640
people

1655
01:01:51,839 --> 01:01:58,319
interpret that as there's some kind of

1656
01:01:54,640 --> 01:02:01,760
inner theater called the mind

1657
01:01:58,319 --> 01:02:04,319
and only I can see what's in my mind and

1658
01:02:01,760 --> 01:02:06,640
in this inner theata there's little pink

1659
01:02:04,319 --> 01:02:08,079
elephants floating around.

1660
01:02:06,640 --> 01:02:10,640
So in other words, what's happened is my

1661
01:02:08,079 --> 01:02:13,280
perceptual systems gone wrong and I'm

1662
01:02:10,640 --> 01:02:15,200
trying to indicate to you how it's gone

1663
01:02:13,280 --> 01:02:17,839
wrong and what it's trying to tell me.

1664
01:02:15,200 --> 01:02:19,520
And the way I do that is by telling you

1665
01:02:17,839 --> 01:02:22,960
what would have to be out there in the

1666
01:02:19,520 --> 01:02:24,880
real world for it to be telling the

1667
01:02:22,960 --> 01:02:27,040
truth.

1668
01:02:24,880 --> 01:02:29,520
And so these little pink elephants,

1669
01:02:27,040 --> 01:02:31,359
they're not in some inner theater. These

1670
01:02:29,520 --> 01:02:33,839
little pink elephants are hypothetical

1671
01:02:31,359 --> 01:02:36,000
things in the real world. And that's my

1672
01:02:33,839 --> 01:02:38,799
way of telling you how my perceptual

1673
01:02:36,000 --> 01:02:41,599
systems telling me FIPS. So now let's do

1674
01:02:38,799 --> 01:02:43,760
that with a chatbot. Yeah. because I

1675
01:02:41,599 --> 01:02:46,720
believe that current multimodal chatbots

1676
01:02:43,760 --> 01:02:48,880
have subjective experiences and very few

1677
01:02:46,720 --> 01:02:51,280
people believe that. But I'll try and

1678
01:02:48,880 --> 01:02:53,520
make you believe it. So suppose I have a

1679
01:02:51,280 --> 01:02:55,680
multimodal chatbot. It's got a robot arm

1680
01:02:53,520 --> 01:02:58,079
so it can point and it's got a camera so

1681
01:02:55,680 --> 01:03:00,079
it can see things and I put an object in

1682
01:02:58,079 --> 01:03:03,599
front of it and I say point at the

1683
01:03:00,079 --> 01:03:06,160
object. It goes like this. No problem.

1684
01:03:03,599 --> 01:03:07,760
Then I put a prism in front of its lens.

1685
01:03:06,160 --> 01:03:09,760
And so then I put an object in front of

1686
01:03:07,760 --> 01:03:11,760
it and I say point at the object and it

1687
01:03:09,760 --> 01:03:14,000
goes there.

1688
01:03:11,760 --> 01:03:15,359
And I say, "No, that's not where the

1689
01:03:14,000 --> 01:03:17,200
object is. The object's actually

1690
01:03:15,359 --> 01:03:19,440
straight in front of you, but I put a

1691
01:03:17,200 --> 01:03:21,839
prism in front of your lens." And the

1692
01:03:19,440 --> 01:03:24,880
chatbot says, "Oh, I see. The prism bent

1693
01:03:21,839 --> 01:03:26,960
the light rays." So, um, the object's

1694
01:03:24,880 --> 01:03:29,599
actually there, but I had the subjective

1695
01:03:26,960 --> 01:03:31,520
experience that it was there.

1696
01:03:29,599 --> 01:03:33,440
Now, if the chatbot says that, is using

1697
01:03:31,520 --> 01:03:35,200
the word subjective experience exactly

1698
01:03:33,440 --> 01:03:37,440
the way people use them. It's an

1699
01:03:35,200 --> 01:03:39,119
alternative view of what's going on.

1700
01:03:37,440 --> 01:03:41,280
They're hypothetical states of the

1701
01:03:39,119 --> 01:03:43,359
world. which if they were true would

1702
01:03:41,280 --> 01:03:44,799
mean my perceptual system wasn't lying.

1703
01:03:43,359 --> 01:03:46,400
And that's the best way I can tell you

1704
01:03:44,799 --> 01:03:49,680
what my perceptual system is doing when

1705
01:03:46,400 --> 01:03:51,039
it's lying to me. Now, we need to go

1706
01:03:49,680 --> 01:03:53,039
further to deal with sentience and

1707
01:03:51,039 --> 01:03:54,400
consciousness and feelings and emotions,

1708
01:03:53,039 --> 01:03:56,000
but I think in the end they're all going

1709
01:03:54,400 --> 01:03:57,440
to be dealt with in a similar way.

1710
01:03:56,000 --> 01:03:59,200
There's no reason machines can't have

1711
01:03:57,440 --> 01:04:02,079
them all because people say machines

1712
01:03:59,200 --> 01:04:04,400
can't have feelings. And people are

1713
01:04:02,079 --> 01:04:06,400
curiously confident about that. I have

1714
01:04:04,400 --> 01:04:09,440
no idea why. Suppose I make a battle

1715
01:04:06,400 --> 01:04:12,240
robot and it's a little battle robot and

1716
01:04:09,440 --> 01:04:14,640
it sees a big battle robot that's much

1717
01:04:12,240 --> 01:04:19,039
more powerful than it. It would be

1718
01:04:14,640 --> 01:04:22,400
really useful if it got scared.

1719
01:04:19,039 --> 01:04:23,920
Now, when I get scared, um, various

1720
01:04:22,400 --> 01:04:25,599
physiological things happen that we

1721
01:04:23,920 --> 01:04:27,839
don't need to go into, and those won't

1722
01:04:25,599 --> 01:04:29,599
happen with the robot. But all the

1723
01:04:27,839 --> 01:04:32,799
cognitive things like I better get the

1724
01:04:29,599 --> 01:04:35,599
hell out of here and I better sort of

1725
01:04:32,799 --> 01:04:36,720
change my way of thinking so I focus and

1726
01:04:35,599 --> 01:04:39,520
focus and focus and don't get

1727
01:04:36,720 --> 01:04:42,640
distracted. All of that will happen with

1728
01:04:39,520 --> 01:04:45,280
robots, too. People will build in things

1729
01:04:42,640 --> 01:04:46,319
so that they when the circumstances such

1730
01:04:45,280 --> 01:04:48,559
they should get the hell out of there,

1731
01:04:46,319 --> 01:04:51,039
they get scared and run away. They'll

1732
01:04:48,559 --> 01:04:53,280
have emotions then. They won't have the

1733
01:04:51,039 --> 01:04:55,520
physiological aspects, but they will

1734
01:04:53,280 --> 01:04:56,799
have all the cognitive aspects. And I

1735
01:04:55,520 --> 01:04:58,480
think it would be odd to say they're

1736
01:04:56,799 --> 01:05:00,000
just simulating emotions. No, they're

1737
01:04:58,480 --> 01:05:02,799
really having those emotions. The little

1738
01:05:00,000 --> 01:05:04,160
robot got scared and ran away. It's not

1739
01:05:02,799 --> 01:05:06,640
running away because of adrenaline. It's

1740
01:05:04,160 --> 01:05:09,039
running away because of a sequence of

1741
01:05:06,640 --> 01:05:10,960
sort of neurological in its neural net

1742
01:05:09,039 --> 01:05:13,520
processes happened which which have the

1743
01:05:10,960 --> 01:05:15,039
equivalent effect to adrenaline. So do

1744
01:05:13,520 --> 01:05:16,319
you do you and it's not just adrenaline,

1745
01:05:15,039 --> 01:05:18,480
right? There's a lot of cognitive stuff

1746
01:05:16,319 --> 01:05:21,599
goes on when you get scared. Yeah. So,

1747
01:05:18,480 --> 01:05:24,240
do you think that

1748
01:05:21,599 --> 01:05:26,799
there is conscious AI? And when I say

1749
01:05:24,240 --> 01:05:28,240
conscious, I mean that represents the

1750
01:05:26,799 --> 01:05:30,480
same properties of consciousness that a

1751
01:05:28,240 --> 01:05:31,920
human has. There's two issues here.

1752
01:05:30,480 --> 01:05:33,760
There's a sort of empirical one and a

1753
01:05:31,920 --> 01:05:36,000
philosophical one. I don't think there's

1754
01:05:33,760 --> 01:05:38,559
anything in principle that stops

1755
01:05:36,000 --> 01:05:39,839
machines from being conscious.

1756
01:05:38,559 --> 01:05:42,240
I'll give you a little demonstration of

1757
01:05:39,839 --> 01:05:44,960
that before we carry on. Suppose I take

1758
01:05:42,240 --> 01:05:47,920
your brain and I take one brain cell in

1759
01:05:44,960 --> 01:05:50,240
your brain and I replace it by this a

1760
01:05:47,920 --> 01:05:52,400
bit black mirror-l like. I replace it by

1761
01:05:50,240 --> 01:05:55,200
a little piece of nanotechnology that's

1762
01:05:52,400 --> 01:05:56,960
just the same size that behaves in

1763
01:05:55,200 --> 01:05:58,799
exactly the same way when it gets pings

1764
01:05:56,960 --> 01:06:00,799
from other neurons. It sends out pings

1765
01:05:58,799 --> 01:06:02,319
just as the brain cell would have. So

1766
01:06:00,799 --> 01:06:04,000
the other neurons don't know anything's

1767
01:06:02,319 --> 01:06:05,760
changed.

1768
01:06:04,000 --> 01:06:07,039
Okay. I've just replaced one of your

1769
01:06:05,760 --> 01:06:08,960
brain cells with this little piece of

1770
01:06:07,039 --> 01:06:10,240
nanote technology. Would you still be

1771
01:06:08,960 --> 01:06:12,079
conscious?

1772
01:06:10,240 --> 01:06:13,920
Yeah. Now you can see where this

1773
01:06:12,079 --> 01:06:16,160
argument is going. Yeah. So if you

1774
01:06:13,920 --> 01:06:17,520
replaced all of them as I replace them

1775
01:06:16,160 --> 01:06:19,839
all, at what point do you stop being

1776
01:06:17,520 --> 01:06:22,079
conscious? Well, people think of

1777
01:06:19,839 --> 01:06:24,720
consciousness as this like ethereal

1778
01:06:22,079 --> 01:06:26,880
thing that exists maybe beyond the brain

1779
01:06:24,720 --> 01:06:29,119
cells. Yeah. Well, people have a lot of

1780
01:06:26,880 --> 01:06:31,359
crazy ideas.

1781
01:06:29,119 --> 01:06:32,960
Um, people don't know what consciousness

1782
01:06:31,359 --> 01:06:35,760
is and they often don't know what they

1783
01:06:32,960 --> 01:06:37,920
mean by it. And then they fall back on

1784
01:06:35,760 --> 01:06:39,839
saying, well, I know it cuz I've got it

1785
01:06:37,920 --> 01:06:41,520
and I can see that I've got it and they

1786
01:06:39,839 --> 01:06:44,240
fall back on this theata model of the

1787
01:06:41,520 --> 01:06:45,680
mind which I think is nonsense. What do

1788
01:06:44,240 --> 01:06:47,119
you think of consciousness as if you had

1789
01:06:45,680 --> 01:06:48,400
to try and define it? Is it because I

1790
01:06:47,119 --> 01:06:51,280
think of it as just like the awareness

1791
01:06:48,400 --> 01:06:53,920
of myself? I don't know. I think it's a

1792
01:06:51,280 --> 01:06:56,640
term we'll stop using. Suppose you want

1793
01:06:53,920 --> 01:06:58,559
to understand how a car works. Well, you

1794
01:06:56,640 --> 01:07:00,720
know, some cars have a lot of oomph and

1795
01:06:58,559 --> 01:07:03,920
other cars have a lot less oomph. Like

1796
01:07:00,720 --> 01:07:05,599
an Aston Martin's got lots of oomph. And

1797
01:07:03,920 --> 01:07:08,480
a little Toyota Corolla doesn't have

1798
01:07:05,599 --> 01:07:11,280
much oomph. But oomph isn't a very good

1799
01:07:08,480 --> 01:07:12,799
concept for understanding cars. Um, if

1800
01:07:11,280 --> 01:07:14,559
you want to understand cars, you need to

1801
01:07:12,799 --> 01:07:17,039
understand about electric engines or

1802
01:07:14,559 --> 01:07:19,760
petrol engines and how they work. And it

1803
01:07:17,039 --> 01:07:21,359
gives rise to oomph, but oomph isn't a

1804
01:07:19,760 --> 01:07:23,039
very useful explanatory concept. It's a

1805
01:07:21,359 --> 01:07:25,119
kind of essence of a car. It's the

1806
01:07:23,039 --> 01:07:26,480
essence of an Aston Martin, but it

1807
01:07:25,119 --> 01:07:28,799
doesn't explain much. I think

1808
01:07:26,480 --> 01:07:31,039
consciousness is like that. And I think

1809
01:07:28,799 --> 01:07:33,520
we'll stop using that term, but I don't

1810
01:07:31,039 --> 01:07:36,480
think there's anything any reason why a

1811
01:07:33,520 --> 01:07:37,920
machine shouldn't have it. If your view

1812
01:07:36,480 --> 01:07:40,720
of consciousness is that it

1813
01:07:37,920 --> 01:07:41,839
intrinsically involves self-awareness,

1814
01:07:40,720 --> 01:07:43,039
then the machine's got to have

1815
01:07:41,839 --> 01:07:44,720
self-awareness. He's got to have

1816
01:07:43,039 --> 01:07:47,359
cognition about its own cognition and

1817
01:07:44,720 --> 01:07:50,400
stuff. But

1818
01:07:47,359 --> 01:07:52,880
I'm a materialist through and through.

1819
01:07:50,400 --> 01:07:54,799
And I don't think there's any reason why

1820
01:07:52,880 --> 01:07:56,799
a machine shouldn't have consciousness.

1821
01:07:54,799 --> 01:07:59,039
Do you think they do then have the same

1822
01:07:56,799 --> 01:08:02,559
consciousness that we think of ourselves

1823
01:07:59,039 --> 01:08:05,039
as being uniquely uh given as a gift

1824
01:08:02,559 --> 01:08:08,559
when we're born? I'm ambivalent about

1825
01:08:05,039 --> 01:08:10,400
that at present. So

1826
01:08:08,559 --> 01:08:12,480
I don't think there's this hard line. I

1827
01:08:10,400 --> 01:08:14,880
think as soon as you have a machine that

1828
01:08:12,480 --> 01:08:18,000
has some self-awareness,

1829
01:08:14,880 --> 01:08:19,839
it's got some consciousness. Um, I think

1830
01:08:18,000 --> 01:08:22,560
it's an emergent property of a complex

1831
01:08:19,839 --> 01:08:24,080
system. It's not a sort of essence

1832
01:08:22,560 --> 01:08:26,080
that's

1833
01:08:24,080 --> 01:08:28,159
throughout the universe. It's you make

1834
01:08:26,080 --> 01:08:29,440
this really complicated system that's

1835
01:08:28,159 --> 01:08:30,960
complicated enough to have a model of

1836
01:08:29,440 --> 01:08:34,560
itself

1837
01:08:30,960 --> 01:08:36,640
and it does perception. And I think then

1838
01:08:34,560 --> 01:08:38,000
you're beginning to get a conscious

1839
01:08:36,640 --> 01:08:39,679
machines. So I don't think there's any

1840
01:08:38,000 --> 01:08:42,000
sharp distinction between what we've got

1841
01:08:39,679 --> 01:08:43,040
now and conscious machines. I don't

1842
01:08:42,000 --> 01:08:45,199
think it's going to one day we're going

1843
01:08:43,040 --> 01:08:46,880
to wake up and say, "Hey, if you put

1844
01:08:45,199 --> 01:08:48,159
this special chemical in, it becomes

1845
01:08:46,880 --> 01:08:50,560
conscious." It's not going to be like

1846
01:08:48,159 --> 01:08:53,040
that. I think we all wonder if these

1847
01:08:50,560 --> 01:08:55,120
computers are like thinking like we are

1848
01:08:53,040 --> 01:08:56,799
on their own when we're not there. And

1849
01:08:55,120 --> 01:08:58,719
if they're experiencing emotions, if

1850
01:08:56,799 --> 01:08:59,600
they're contending with I think we

1851
01:08:58,719 --> 01:09:01,040
probably, you know, we think about

1852
01:08:59,600 --> 01:09:04,400
things like love and things that are

1853
01:09:01,040 --> 01:09:07,440
feel unique to biological species. Um,

1854
01:09:04,400 --> 01:09:09,120
are they sat there thinking? Are they do

1855
01:09:07,440 --> 01:09:11,359
they have concerns? I think they really

1856
01:09:09,120 --> 01:09:14,640
are thinking and I think as soon as you

1857
01:09:11,359 --> 01:09:16,080
make AI agents they will have concerns.

1858
01:09:14,640 --> 01:09:18,000
If you wanted to make an effective AI

1859
01:09:16,080 --> 01:09:21,120
agent suppose you let's take a call

1860
01:09:18,000 --> 01:09:23,359
center. In a call center you have people

1861
01:09:21,120 --> 01:09:26,239
at present they have all sorts of

1862
01:09:23,359 --> 01:09:28,719
emotions and feelings which are kind of

1863
01:09:26,239 --> 01:09:32,319
useful. So suppose I call up the call

1864
01:09:28,719 --> 01:09:34,480
center and I'm actually lonely and I

1865
01:09:32,319 --> 01:09:36,799
don't actually want to know the answer

1866
01:09:34,480 --> 01:09:40,319
to why my computer isn't working. I just

1867
01:09:36,799 --> 01:09:42,799
want somebody to talk to. After a while,

1868
01:09:40,319 --> 01:09:45,279
the person in the call center will

1869
01:09:42,799 --> 01:09:47,839
either get bored or get annoyed with me

1870
01:09:45,279 --> 01:09:50,719
and will terminate it.

1871
01:09:47,839 --> 01:09:52,319
Well, you replace them by an AI agent.

1872
01:09:50,719 --> 01:09:54,480
The AI agent needs to have the same kind

1873
01:09:52,319 --> 01:09:55,760
of responses. If someone's just called

1874
01:09:54,480 --> 01:09:58,080
up because they just want to talk to the

1875
01:09:55,760 --> 01:10:00,080
AI agent and we're happy to talk for the

1876
01:09:58,080 --> 01:10:02,239
whole day to the AI agent, that's not

1877
01:10:00,080 --> 01:10:03,920
good for business. And you want an AI

1878
01:10:02,239 --> 01:10:06,159
agent that either gets bored or gets

1879
01:10:03,920 --> 01:10:08,400
irritated and says, "I'm sorry, but I

1880
01:10:06,159 --> 01:10:12,000
don't have time for this." And once it

1881
01:10:08,400 --> 01:10:14,640
does that, I think it's got emotions.

1882
01:10:12,000 --> 01:10:16,400
Now, like I say, emotions have two

1883
01:10:14,640 --> 01:10:18,880
aspects to them. There's the cognitive

1884
01:10:16,400 --> 01:10:21,120
aspect and the behavioral aspect, and

1885
01:10:18,880 --> 01:10:25,120
then there's a physiological aspect, and

1886
01:10:21,120 --> 01:10:27,440
those go together with us. And if the AI

1887
01:10:25,120 --> 01:10:29,440
agent gets embarrassed, it won't go red.

1888
01:10:27,440 --> 01:10:31,440
Yeah. Um, so there's no physiological

1889
01:10:29,440 --> 01:10:33,040
skin won't start sweating. Yeah, but it

1890
01:10:31,440 --> 01:10:34,159
might have all the same behavior. And in

1891
01:10:33,040 --> 01:10:36,000
that case, I'd say yeah, it's having

1892
01:10:34,159 --> 01:10:37,760
emotion. It's got an emotion. So, it's

1893
01:10:36,000 --> 01:10:39,600
going to have the same sort of cognitive

1894
01:10:37,760 --> 01:10:41,679
thought and then it's going to act upon

1895
01:10:39,600 --> 01:10:43,920
that cognitive in the same way, but

1896
01:10:41,679 --> 01:10:45,840
without the physiological responses. And

1897
01:10:43,920 --> 01:10:47,520
does that matter that it doesn't go red

1898
01:10:45,840 --> 01:10:48,880
in the face? And it's just a different I

1899
01:10:47,520 --> 01:10:51,120
mean, that's a response to the It makes

1900
01:10:48,880 --> 01:10:53,120
it somewhat different from us. Yeah. For

1901
01:10:51,120 --> 01:10:55,840
some things, the physiological aspects

1902
01:10:53,120 --> 01:10:57,280
are very important like love. They're a

1903
01:10:55,840 --> 01:11:00,320
long way from having love the same way

1904
01:10:57,280 --> 01:11:02,320
we do. But I don't see why they

1905
01:11:00,320 --> 01:11:05,440
shouldn't have emotions. So I think

1906
01:11:02,320 --> 01:11:07,840
what's happened is people have a model

1907
01:11:05,440 --> 01:11:10,159
of how the mind works and what feelings

1908
01:11:07,840 --> 01:11:13,360
are and what emotions are and their

1909
01:11:10,159 --> 01:11:16,080
model is just wrong. What um what

1910
01:11:13,360 --> 01:11:18,239
brought you to Google? You you worked at

1911
01:11:16,080 --> 01:11:21,920
Google for about a decade, right? Yeah.

1912
01:11:18,239 --> 01:11:24,159
What brought you there? I have a son who

1913
01:11:21,920 --> 01:11:26,640
has learning difficulties

1914
01:11:24,159 --> 01:11:30,159
and in order to be sure he would never

1915
01:11:26,640 --> 01:11:32,800
be out on the street, I needed to get

1916
01:11:30,159 --> 01:11:35,280
several million dollars and I wasn't

1917
01:11:32,800 --> 01:11:38,080
going to get that as an academic. I

1918
01:11:35,280 --> 01:11:39,600
tried. So, I taught a Corsera course in

1919
01:11:38,080 --> 01:11:40,800
the hope that I'd make lots of money

1920
01:11:39,600 --> 01:11:43,679
that way, but there was no money in

1921
01:11:40,800 --> 01:11:46,800
that. Mhm. So I figured out well the

1922
01:11:43,679 --> 01:11:51,440
only way to get millions of dollars is

1923
01:11:46,800 --> 01:11:54,800
to sell myself to a big company.

1924
01:11:51,440 --> 01:11:57,120
And so when I was 65,

1925
01:11:54,800 --> 01:11:59,040
fortunately for me, I had two brilliant

1926
01:11:57,120 --> 01:12:01,600
students who produced something called

1927
01:11:59,040 --> 01:12:03,040
Alexet, which was neural net that was

1928
01:12:01,600 --> 01:12:06,159
very good at recognizing objects in

1929
01:12:03,040 --> 01:12:10,159
images. And

1930
01:12:06,159 --> 01:12:12,640
so Ilia and Alex and I set up a little

1931
01:12:10,159 --> 01:12:14,480
company and auctioned it. And we

1932
01:12:12,640 --> 01:12:16,159
actually set up an auction where we had

1933
01:12:14,480 --> 01:12:18,000
a number of big companies bidding for

1934
01:12:16,159 --> 01:12:21,440
us.

1935
01:12:18,000 --> 01:12:23,760
And that company was called AlexNet. No,

1936
01:12:21,440 --> 01:12:25,679
the the the network that recognized

1937
01:12:23,760 --> 01:12:27,760
objects was called Alexet. The company

1938
01:12:25,679 --> 01:12:29,840
was called DNN Research, deep neural

1939
01:12:27,760 --> 01:12:31,360
network research. And it was doing

1940
01:12:29,840 --> 01:12:33,199
things like this. I'll put this graph up

1941
01:12:31,360 --> 01:12:37,199
on the screen. That's that's Alexet.

1942
01:12:33,199 --> 01:12:39,199
This picture shows eight images and Alex

1943
01:12:37,199 --> 01:12:40,800
Net's ability, which is your company's

1944
01:12:39,199 --> 01:12:43,360
ability to spot what was in those

1945
01:12:40,800 --> 01:12:44,640
images. Yeah. So, it could tell the

1946
01:12:43,360 --> 01:12:48,800
difference between various kinds of

1947
01:12:44,640 --> 01:12:51,360
mushroom. And about 12% of imageet is

1948
01:12:48,800 --> 01:12:53,199
dogs. And to be good at imageet, you

1949
01:12:51,360 --> 01:12:55,520
have to tell the difference between very

1950
01:12:53,199 --> 01:12:57,920
similar kinds of dog. And it would got

1951
01:12:55,520 --> 01:13:00,960
to be very good at that. And your your

1952
01:12:57,920 --> 01:13:02,880
company Alexet won several awards I

1953
01:13:00,960 --> 01:13:04,880
believe for its ability to out

1954
01:13:02,880 --> 01:13:07,360
outperform its competitors. And so

1955
01:13:04,880 --> 01:13:09,440
Google ultimately ended up acquiring

1956
01:13:07,360 --> 01:13:12,080
your technology. Google acquired that

1957
01:13:09,440 --> 01:13:14,159
technology and some other technology.

1958
01:13:12,080 --> 01:13:17,840
And you went to work at Google at age

1959
01:13:14,159 --> 01:13:21,840
what 66. I went at age 65 to work at

1960
01:13:17,840 --> 01:13:23,199
Google. 65. And you left at age 76? 75.

1961
01:13:21,840 --> 01:13:25,040
75. Okay. I worked there for more or

1962
01:13:23,199 --> 01:13:27,120
less exactly 10 years. And what were you

1963
01:13:25,040 --> 01:13:29,440
doing there? Okay, they were very nice

1964
01:13:27,120 --> 01:13:31,760
to me. They said they said pretty much

1965
01:13:29,440 --> 01:13:33,360
you can do what you like. I worked on

1966
01:13:31,760 --> 01:13:35,360
something called distillation that did

1967
01:13:33,360 --> 01:13:38,239
really work well

1968
01:13:35,360 --> 01:13:40,400
and that's now used all the time in AI

1969
01:13:38,239 --> 01:13:42,560
in AI and distillation is a way of

1970
01:13:40,400 --> 01:13:44,400
taking what a big model knows a big

1971
01:13:42,560 --> 01:13:46,560
neural net knows and getting that

1972
01:13:44,400 --> 01:13:48,719
knowledge into a small neural net. Then

1973
01:13:46,560 --> 01:13:50,960
at the end I got very interested in

1974
01:13:48,719 --> 01:13:52,560
analog computation and whether it would

1975
01:13:50,960 --> 01:13:55,840
be possible to get these big language

1976
01:13:52,560 --> 01:13:58,239
models running in analog hardware. So

1977
01:13:55,840 --> 01:14:00,640
they used much less energy. And it was

1978
01:13:58,239 --> 01:14:02,480
when I was doing that work that I began

1979
01:14:00,640 --> 01:14:05,520
to really realize how much better

1980
01:14:02,480 --> 01:14:08,640
digital is for sharing information.

1981
01:14:05,520 --> 01:14:11,600
Was there a Eureka moment?

1982
01:14:08,640 --> 01:14:14,320
There was a Eureka month or two. Um and

1983
01:14:11,600 --> 01:14:15,840
it was a sort of coupling of chat beauty

1984
01:14:14,320 --> 01:14:19,040
coming out although Google had very

1985
01:14:15,840 --> 01:14:20,800
similar things a year earlier and I'd

1986
01:14:19,040 --> 01:14:23,120
seen those and that had a big effect

1987
01:14:20,800 --> 01:14:26,880
effect on me. The closest I had to a

1988
01:14:23,120 --> 01:14:29,920
Eureka moment was when a Google system

1989
01:14:26,880 --> 01:14:31,920
called Palm was able to say why a joke

1990
01:14:29,920 --> 01:14:33,840
was funny. And I'd always thought of

1991
01:14:31,920 --> 01:14:35,760
that as a kind of landmark. If it can

1992
01:14:33,840 --> 01:14:38,239
say why a joke's funny, it really does

1993
01:14:35,760 --> 01:14:41,480
understand and it could say why a joke

1994
01:14:38,239 --> 01:14:41,480
was funny.

1995
01:14:41,679 --> 01:14:45,199
And that coupled with realizing why

1996
01:14:43,600 --> 01:14:47,280
digital is so much better than analog

1997
01:14:45,199 --> 01:14:50,480
for sharing information

1998
01:14:47,280 --> 01:14:53,199
suddenly made me very interested in AI

1999
01:14:50,480 --> 01:14:56,000
safety and that these things were going

2000
01:14:53,199 --> 01:14:58,880
to get a lot smarter than us. Why did

2001
01:14:56,000 --> 01:15:01,840
you leave Google? The main reason I left

2002
01:14:58,880 --> 01:15:04,159
Google was cuz I was 75 and I wanted to

2003
01:15:01,840 --> 01:15:07,120
retire. I've done a very bad job of

2004
01:15:04,159 --> 01:15:09,199
that. The precise timing of when I left

2005
01:15:07,120 --> 01:15:11,920
Google was so that I could talk freely

2006
01:15:09,199 --> 01:15:15,520
at a conference at MIT, but I left

2007
01:15:11,920 --> 01:15:17,360
because I was I'm old and I was finding

2008
01:15:15,520 --> 01:15:18,800
it harder to program. I was making many

2009
01:15:17,360 --> 01:15:20,560
more mistakes when I programmed, which

2010
01:15:18,800 --> 01:15:23,199
is very annoying. You wanted to talk

2011
01:15:20,560 --> 01:15:25,520
freely at a conference at MIT. Yes. At

2012
01:15:23,199 --> 01:15:27,120
MIT, organized by MIT Tech Review. What

2013
01:15:25,520 --> 01:15:28,880
did you want to talk about freely? AI

2014
01:15:27,120 --> 01:15:31,120
safety. And you couldn't do that while

2015
01:15:28,880 --> 01:15:32,159
you were at Google. Well, I could have

2016
01:15:31,120 --> 01:15:33,679
done it while I was at Google. And

2017
01:15:32,159 --> 01:15:35,760
Google encouraged me to stay and work on

2018
01:15:33,679 --> 01:15:38,560
AI safety and said I could do whatever I

2019
01:15:35,760 --> 01:15:40,880
liked on AI safety. You kind of sense to

2020
01:15:38,560 --> 01:15:43,520
yourself if you work for a big company.

2021
01:15:40,880 --> 01:15:45,920
You don't feel right saying things that

2022
01:15:43,520 --> 01:15:47,360
will damage the big company. Even if you

2023
01:15:45,920 --> 01:15:50,080
could get away with it, it just feels

2024
01:15:47,360 --> 01:15:51,360
wrong to me. I didn't leave because I

2025
01:15:50,080 --> 01:15:52,719
was cross with anything Google was

2026
01:15:51,360 --> 01:15:54,800
doing. I think Google actually behaved

2027
01:15:52,719 --> 01:15:57,760
very responsibly. When they had these

2028
01:15:54,800 --> 01:15:59,120
big chat bots, they didn't release them

2029
01:15:57,760 --> 01:16:01,440
possibly because they were worried about

2030
01:15:59,120 --> 01:16:02,800
their reputation. they had a very good

2031
01:16:01,440 --> 01:16:05,679
reputation and they didn't want to

2032
01:16:02,800 --> 01:16:07,760
damage it. So open AI didn't have a

2033
01:16:05,679 --> 01:16:09,520
reputation and so they could afford to

2034
01:16:07,760 --> 01:16:11,520
take the gamble. I mean there's also a

2035
01:16:09,520 --> 01:16:13,120
big conversation happening around how it

2036
01:16:11,520 --> 01:16:16,719
will cannibalize their core business in

2037
01:16:13,120 --> 01:16:18,560
search. There is now. Yes. Yeah. Yeah.

2038
01:16:16,719 --> 01:16:20,800
And it's the old innovators dilemas to

2039
01:16:18,560 --> 01:16:23,280
some degree I guess that contending with

2040
01:16:20,800 --> 01:16:25,679
bad skin. I've had it and I'm sure many

2041
01:16:23,280 --> 01:16:28,719
of you listening have had it too or

2042
01:16:25,679 --> 01:16:30,000
maybe you have it right now. I know how

2043
01:16:28,719 --> 01:16:31,920
draining it can be, especially if you're

2044
01:16:30,000 --> 01:16:33,600
in a job where you're presenting often

2045
01:16:31,920 --> 01:16:35,120
like I am. So, let me tell you about

2046
01:16:33,600 --> 01:16:37,520
something that's helped both my partner

2047
01:16:35,120 --> 01:16:39,280
and me and my sister, which is red light

2048
01:16:37,520 --> 01:16:41,440
therapy. I only got into this a couple

2049
01:16:39,280 --> 01:16:43,120
of years ago, but I wish I'd known a

2050
01:16:41,440 --> 01:16:45,679
little bit sooner. I've been using our

2051
01:16:43,120 --> 01:16:47,679
show sponsors Boncharg's infrared sauna

2052
01:16:45,679 --> 01:16:49,600
blanket for a while now, but I just got

2053
01:16:47,679 --> 01:16:51,440
hold of their red light therapy mask as

2054
01:16:49,600 --> 01:16:53,600
well. Red light has been proven to have

2055
01:16:51,440 --> 01:16:55,520
so many benefits for the body. Like any

2056
01:16:53,600 --> 01:16:57,440
area of your skin that's exposed will

2057
01:16:55,520 --> 01:16:59,920
see a reduction in scarring, wrinkles,

2058
01:16:57,440 --> 01:17:01,920
and even blemishes. It also helps with

2059
01:16:59,920 --> 01:17:03,520
complexion. It boosts collagen, and it

2060
01:17:01,920 --> 01:17:05,280
does that by targeting the upper layers

2061
01:17:03,520 --> 01:17:07,440
of your skin. And Boncharge ships

2062
01:17:05,280 --> 01:17:09,280
worldwide with easy returns and a

2063
01:17:07,440 --> 01:17:10,800
year-long warranty on all of their

2064
01:17:09,280 --> 01:17:11,600
products. So, if you'd like to try it

2065
01:17:10,800 --> 01:17:13,600
yourself, head over to

2066
01:17:11,600 --> 01:17:16,880
bondcharge.com/diary

2067
01:17:13,600 --> 01:17:18,960
and use code diary for 25% off any

2068
01:17:16,880 --> 01:17:20,080
product sitewide. Just make sure you

2069
01:17:18,960 --> 01:17:22,400
order through this link.

2070
01:17:20,080 --> 01:17:24,960
bondcharge.com/diary

2071
01:17:22,400 --> 01:17:26,960
with code diary. Make sure you keep what

2072
01:17:24,960 --> 01:17:29,120
I'm about to say to yourself. I'm

2073
01:17:26,960 --> 01:17:31,120
inviting 10,000 of you to come even

2074
01:17:29,120 --> 01:17:34,400
deeper into the diary of a CEO. Welcome

2075
01:17:31,120 --> 01:17:35,840
to my inner circle. This is a brand new

2076
01:17:34,400 --> 01:17:37,440
private community that I'm launching to

2077
01:17:35,840 --> 01:17:39,840
the world. We have so many incredible

2078
01:17:37,440 --> 01:17:41,679
things that happen that you are never

2079
01:17:39,840 --> 01:17:42,880
shown. We have the briefs that are on my

2080
01:17:41,679 --> 01:17:44,960
iPad when I'm recording the

2081
01:17:42,880 --> 01:17:46,400
conversation. We have clips we've never

2082
01:17:44,960 --> 01:17:47,920
released. We have behindthe-scenes

2083
01:17:46,400 --> 01:17:50,239
conversations with the guests. and also

2084
01:17:47,920 --> 01:17:53,520
the episodes that we've never ever

2085
01:17:50,239 --> 01:17:55,280
released and so much more. In the

2086
01:17:53,520 --> 01:17:56,880
circle, you'll have direct access to me.

2087
01:17:55,280 --> 01:17:58,640
You can tell us what you want this show

2088
01:17:56,880 --> 01:18:00,159
to be, who you want us to interview, and

2089
01:17:58,640 --> 01:18:02,800
the types of conversations you would

2090
01:18:00,159 --> 01:18:04,800
love us to have. But remember, for now,

2091
01:18:02,800 --> 01:18:07,040
we're only inviting the first 10,000

2092
01:18:04,800 --> 01:18:08,320
people that join before it closes. So,

2093
01:18:07,040 --> 01:18:09,520
if you want to join our private closed

2094
01:18:08,320 --> 01:18:10,640
community, head to the link in the

2095
01:18:09,520 --> 01:18:13,640
description below or go to

2096
01:18:10,640 --> 01:18:13,640
daccircle.com.

2097
01:18:14,159 --> 01:18:17,760
I will speak to you there.

2098
01:18:16,080 --> 01:18:19,040
I'm continually shocked by the types of

2099
01:18:17,760 --> 01:18:20,800
individuals that listen to this

2100
01:18:19,040 --> 01:18:21,840
conversation um because they come up to

2101
01:18:20,800 --> 01:18:23,360
me sometimes. So I hear from

2102
01:18:21,840 --> 01:18:24,800
politicians, I hear from some real

2103
01:18:23,360 --> 01:18:26,239
people, I hear from entrepreneurs all

2104
01:18:24,800 --> 01:18:27,199
over the world, whether they are the

2105
01:18:26,239 --> 01:18:28,960
entrepreneurs building some of the

2106
01:18:27,199 --> 01:18:31,679
biggest companies in the world or their,

2107
01:18:28,960 --> 01:18:33,679
you know, early stage startups. For

2108
01:18:31,679 --> 01:18:35,760
those people that are listening to this

2109
01:18:33,679 --> 01:18:38,080
conversation now that are in positions

2110
01:18:35,760 --> 01:18:40,400
of power and influence,

2111
01:18:38,080 --> 01:18:42,320
world leaders, let's say, what's your

2112
01:18:40,400 --> 01:18:44,080
message to them?

2113
01:18:42,320 --> 01:18:45,840
I'd say what you need is highly

2114
01:18:44,080 --> 01:18:47,760
regulated capitalism. That's what seems

2115
01:18:45,840 --> 01:18:49,920
to work best. And what would you say to

2116
01:18:47,760 --> 01:18:51,840
the average person

2117
01:18:49,920 --> 01:18:54,239
not doesn't work in the industry,

2118
01:18:51,840 --> 01:18:56,480
somewhat concerned about the future,

2119
01:18:54,239 --> 01:18:57,679
doesn't know if they're helpless or not.

2120
01:18:56,480 --> 01:18:59,520
What should they be doing in their own

2121
01:18:57,679 --> 01:19:01,520
lives?

2122
01:18:59,520 --> 01:19:04,080
My feeling is there's not much they can

2123
01:19:01,520 --> 01:19:06,800
do. This isn't isn't going to be decided

2124
01:19:04,080 --> 01:19:09,360
by just as climate change isn't going to

2125
01:19:06,800 --> 01:19:12,640
be decided by people separating out the

2126
01:19:09,360 --> 01:19:14,400
plastic bags from the um compostables.

2127
01:19:12,640 --> 01:19:16,960
That's not going to have much effect.

2128
01:19:14,400 --> 01:19:18,960
It's going to be decided by whether the

2129
01:19:16,960 --> 01:19:21,040
lobbyists for the big energy companies

2130
01:19:18,960 --> 01:19:26,800
can be kept under control. I don't think

2131
01:19:21,040 --> 01:19:30,719
there's much people can do to except for

2132
01:19:26,800 --> 01:19:32,560
try and pressure their governments to

2133
01:19:30,719 --> 01:19:36,719
force the big companies to work on AI

2134
01:19:32,560 --> 01:19:39,520
safety that they can do.

2135
01:19:36,719 --> 01:19:40,960
You've lived a a fascinating fascinating

2136
01:19:39,520 --> 01:19:42,400
winding life. I think one of the things

2137
01:19:40,960 --> 01:19:45,440
most people don't know about you is that

2138
01:19:42,400 --> 01:19:47,520
your family has a

2139
01:19:45,440 --> 01:19:49,600
big history of being involved in

2140
01:19:47,520 --> 01:19:51,120
tremendous things. You have a family

2141
01:19:49,600 --> 01:19:54,560
tree which is one of the most impressive

2142
01:19:51,120 --> 01:19:57,360
that I've ever seen or read about. Your

2143
01:19:54,560 --> 01:19:59,760
great greatgrandfather George Bull

2144
01:19:57,360 --> 01:20:01,440
founded the Boolean algebra logic which

2145
01:19:59,760 --> 01:20:03,920
is one of the foundational principles of

2146
01:20:01,440 --> 01:20:05,840
modern computer science. You have uh

2147
01:20:03,920 --> 01:20:07,840
your great great grandmother Mary

2148
01:20:05,840 --> 01:20:11,440
Everest Bull who was a mathematician and

2149
01:20:07,840 --> 01:20:13,040
educator who made huge leaps forward in

2150
01:20:11,440 --> 01:20:15,520
mathematics from what I was able to

2151
01:20:13,040 --> 01:20:16,960
ascertain. Um I mean I can the list goes

2152
01:20:15,520 --> 01:20:20,000
on and on and on. I mean, your great

2153
01:20:16,960 --> 01:20:22,480
great uncle George Everest is what Mount

2154
01:20:20,000 --> 01:20:25,520
Everest is named after.

2155
01:20:22,480 --> 01:20:30,719
Is that is that correct? I think he's my

2156
01:20:25,520 --> 01:20:33,280
great great great uncle. His his niece

2157
01:20:30,719 --> 01:20:36,560
married George Bull.

2158
01:20:33,280 --> 01:20:39,520
So Mary Mary Bull was Mary Everest Bull.

2159
01:20:36,560 --> 01:20:41,280
Um she was the niece of Everest. And

2160
01:20:39,520 --> 01:20:43,920
your first cousin once removed, Joan

2161
01:20:41,280 --> 01:20:45,600
Hinton, was involved in the a nuclear

2162
01:20:43,920 --> 01:20:47,199
physicist who worked on the Manhattan

2163
01:20:45,600 --> 01:20:48,960
project, which is the World War II

2164
01:20:47,199 --> 01:20:51,280
development of the first nuclear bomb.

2165
01:20:48,960 --> 01:20:53,760
Yeah. She was one of the two female

2166
01:20:51,280 --> 01:20:56,640
physicists at Los Alamos.

2167
01:20:53,760 --> 01:20:59,840
And then after they dropped the bomb,

2168
01:20:56,640 --> 01:21:02,719
she moved to China. Why? She was very

2169
01:20:59,840 --> 01:21:04,239
cross with them dropping the bomb. And

2170
01:21:02,719 --> 01:21:08,239
her family had a lot of links with

2171
01:21:04,239 --> 01:21:10,960
China. Her mother was friends with

2172
01:21:08,239 --> 01:21:13,040
Chairman Mo.

2173
01:21:10,960 --> 01:21:14,239
Quite weird.

2174
01:21:13,040 --> 01:21:16,560
When you look back at your life,

2175
01:21:14,239 --> 01:21:18,320
Jeffrey,

2176
01:21:16,560 --> 01:21:22,560
we have the hindsight you have now and

2177
01:21:18,320 --> 01:21:24,000
the ret retrospective clarity,

2178
01:21:22,560 --> 01:21:26,480
what might you have done differently if

2179
01:21:24,000 --> 01:21:30,480
you were advising me?

2180
01:21:26,480 --> 01:21:34,400
I guess I have two pieces of advice. One

2181
01:21:30,480 --> 01:21:35,920
is if you have an intuition that people

2182
01:21:34,400 --> 01:21:38,320
are doing things wrong and there's a

2183
01:21:35,920 --> 01:21:40,000
better way to do things, don't give up

2184
01:21:38,320 --> 01:21:42,080
on that intuition just because people

2185
01:21:40,000 --> 01:21:44,719
say it's silly. Don't give up on the

2186
01:21:42,080 --> 01:21:47,040
intuition until you figured out why it's

2187
01:21:44,719 --> 01:21:50,400
wrong. Figured out for yourself why that

2188
01:21:47,040 --> 01:21:52,480
intuition isn't correct. And usually

2189
01:21:50,400 --> 01:21:54,080
it's wrong if it disagrees with

2190
01:21:52,480 --> 01:21:56,639
everybody else and you'll eventually

2191
01:21:54,080 --> 01:21:58,320
figure out why it's wrong.

2192
01:21:56,639 --> 01:22:00,320
But just occasionally you'll have an

2193
01:21:58,320 --> 01:22:02,960
intuition that's actually right and

2194
01:22:00,320 --> 01:22:05,600
everybody else is wrong. And I lucked

2195
01:22:02,960 --> 01:22:08,159
out that way. Early on I thought neural

2196
01:22:05,600 --> 01:22:11,280
nets are definitely the way to go to

2197
01:22:08,159 --> 01:22:13,840
make AI and almost everybody said that

2198
01:22:11,280 --> 01:22:15,520
was crazy and I stuck with it because I

2199
01:22:13,840 --> 01:22:17,440
couldn't. It seemed to me it was

2200
01:22:15,520 --> 01:22:19,840
obviously right.

2201
01:22:17,440 --> 01:22:22,480
Now the idea that you should stick with

2202
01:22:19,840 --> 01:22:24,719
your intuitions isn't going to work if

2203
01:22:22,480 --> 01:22:26,159
you have bad intuitions. But if you have

2204
01:22:24,719 --> 01:22:27,520
bad intuitions, you're never going to do

2205
01:22:26,159 --> 01:22:30,400
anything anyway, so you might as well

2206
01:22:27,520 --> 01:22:32,080
stick with them.

2207
01:22:30,400 --> 01:22:33,440
And in your own career journey, is there

2208
01:22:32,080 --> 01:22:35,440
anything you look back on and say, "With

2209
01:22:33,440 --> 01:22:36,719
the hindsight I have now, I should have

2210
01:22:35,440 --> 01:22:38,880
taken a different approach at that

2211
01:22:36,719 --> 01:22:42,400
juncture."

2212
01:22:38,880 --> 01:22:45,400
I wish I'd spent more time with my wife

2213
01:22:42,400 --> 01:22:45,400
um

2214
01:22:47,280 --> 01:22:50,880
and with my children when they were

2215
01:22:48,719 --> 01:22:55,040
little.

2216
01:22:50,880 --> 01:22:55,040
I was kind of obsessed with work.

2217
01:22:55,360 --> 01:23:00,880
Your wife passed away. Yeah. From

2218
01:22:57,520 --> 01:23:03,760
ovarian cancer. No. Or that was another

2219
01:23:00,880 --> 01:23:05,840
wife. Okay. Um I had two wives to have

2220
01:23:03,760 --> 01:23:07,199
cancer. Oh, really? Sorry. The first one

2221
01:23:05,840 --> 01:23:09,199
died of ovarian cancer and the second

2222
01:23:07,199 --> 01:23:10,719
one died of pancreatic cancer. And you

2223
01:23:09,199 --> 01:23:12,880
wish you'd spent more time with her?

2224
01:23:10,719 --> 01:23:15,199
With the second wife? Yeah. Who was a

2225
01:23:12,880 --> 01:23:17,600
wonderful person?

2226
01:23:15,199 --> 01:23:19,199
Why did you say that in your 70s? What

2227
01:23:17,600 --> 01:23:21,040
is it that you've you figured out that I

2228
01:23:19,199 --> 01:23:22,719
might not know yet?

2229
01:23:21,040 --> 01:23:26,480
Oh, just cuz she's gone and I can't

2230
01:23:22,719 --> 01:23:30,000
spend more time with her now. Mhm.

2231
01:23:26,480 --> 01:23:33,040
But you didn't know that at the time.

2232
01:23:30,000 --> 01:23:35,520
At the time, you think

2233
01:23:33,040 --> 01:23:37,360
I mean it was likely I would die before

2234
01:23:35,520 --> 01:23:40,719
her just cuz she was a woman and I was a

2235
01:23:37,360 --> 01:23:42,320
man. Um I didn't

2236
01:23:40,719 --> 01:23:43,920
I just didn't spend enough time when I

2237
01:23:42,320 --> 01:23:46,239
could.

2238
01:23:43,920 --> 01:23:47,520
I I think I I inquire there because I

2239
01:23:46,239 --> 01:23:48,719
think there's many of us that are so

2240
01:23:47,520 --> 01:23:51,199
consumed with what we're doing

2241
01:23:48,719 --> 01:23:52,800
professionally that we kind of assume

2242
01:23:51,199 --> 01:23:54,719
immortality with our partners because

2243
01:23:52,800 --> 01:23:56,320
they've always been there. So we Yeah. I

2244
01:23:54,719 --> 01:23:59,679
mean she was very supportive of me

2245
01:23:56,320 --> 01:24:01,120
spending a lot of time working but and

2246
01:23:59,679 --> 01:24:02,480
why did you say your children as well?

2247
01:24:01,120 --> 01:24:03,600
What's the what's the Well, I didn't

2248
01:24:02,480 --> 01:24:05,760
spend enough time with them when they

2249
01:24:03,600 --> 01:24:10,280
were little

2250
01:24:05,760 --> 01:24:10,280
and you regret that now. Yeah.

2251
01:24:12,560 --> 01:24:16,639
If you um if you had a closing message

2252
01:24:14,239 --> 01:24:20,400
for for my for my listeners about AI and

2253
01:24:16,639 --> 01:24:22,239
AI safety, what would that be? Jeffrey,

2254
01:24:20,400 --> 01:24:25,360
there's still a chance that we can

2255
01:24:22,239 --> 01:24:27,679
figure out how to develop AI that won't

2256
01:24:25,360 --> 01:24:30,159
want to take over from us. And because

2257
01:24:27,679 --> 01:24:31,760
there's a chance, we should put enormous

2258
01:24:30,159 --> 01:24:33,440
resources into trying to figure that out

2259
01:24:31,760 --> 01:24:36,960
because if we don't, it's going to take

2260
01:24:33,440 --> 01:24:40,400
over. And are you hopeful?

2261
01:24:36,960 --> 01:24:42,719
I just don't know. I'm agnostic.

2262
01:24:40,400 --> 01:24:44,080
you must get get bed get in bed at night

2263
01:24:42,719 --> 01:24:46,719
and when you're thinking to yourself

2264
01:24:44,080 --> 01:24:49,120
about probabilities of outcomes there

2265
01:24:46,719 --> 01:24:50,880
must be a bias in one direction because

2266
01:24:49,120 --> 01:24:53,679
there certainly is for me I imagine

2267
01:24:50,880 --> 01:24:56,239
everyone listening now has a

2268
01:24:53,679 --> 01:24:57,840
internal prediction that they might not

2269
01:24:56,239 --> 01:25:00,480
say out loud but of how they think it's

2270
01:24:57,840 --> 01:25:02,639
going to play out I really don't know I

2271
01:25:00,480 --> 01:25:05,280
genuinely don't know I think it's

2272
01:25:02,639 --> 01:25:08,239
incredibly uncertain when I'm feeling

2273
01:25:05,280 --> 01:25:10,639
slightly depressed I think people are

2274
01:25:08,239 --> 01:25:12,639
toast is going to take over while I'm

2275
01:25:10,639 --> 01:25:14,800
feeling cheerful. I think we'll figure

2276
01:25:12,639 --> 01:25:17,600
out a way. Maybe one of the facets of

2277
01:25:14,800 --> 01:25:19,520
being a human um is because we've always

2278
01:25:17,600 --> 01:25:21,199
been here, like we were saying about our

2279
01:25:19,520 --> 01:25:23,920
loved ones and our relationships, we

2280
01:25:21,199 --> 01:25:25,440
assume casually that we will always be

2281
01:25:23,920 --> 01:25:27,040
here and we'll always figure everything

2282
01:25:25,440 --> 01:25:28,639
out. But there's a beginning and an end

2283
01:25:27,040 --> 01:25:32,960
to everything as we saw from the

2284
01:25:28,639 --> 01:25:35,520
dinosaurs. I mean, yeah. And

2285
01:25:32,960 --> 01:25:39,920
we have to face the possibility

2286
01:25:35,520 --> 01:25:42,560
that unless we do something soon,

2287
01:25:39,920 --> 01:25:43,760
we're near the end.

2288
01:25:42,560 --> 01:25:45,120
We have a closing tradition on this

2289
01:25:43,760 --> 01:25:47,120
podcast where the last guest leaves a

2290
01:25:45,120 --> 01:25:52,120
question in their diary. And the

2291
01:25:47,120 --> 01:25:52,120
question that they've left for you is

2292
01:25:54,800 --> 01:25:58,000
with everything that you see ahead of

2293
01:25:56,320 --> 01:26:00,080
us,

2294
01:25:58,000 --> 01:26:03,400
what is the biggest threat you see to

2295
01:26:00,080 --> 01:26:03,400
human happiness?

2296
01:26:04,719 --> 01:26:09,360
I think the joblessness is a fairly

2297
01:26:07,679 --> 01:26:11,840
urgent short-term threat to human

2298
01:26:09,360 --> 01:26:14,400
happiness. I think if you make lots and

2299
01:26:11,840 --> 01:26:17,760
lots of people unemployed, even if they

2300
01:26:14,400 --> 01:26:19,840
get universal basic income, um they're

2301
01:26:17,760 --> 01:26:22,080
not going to be happy

2302
01:26:19,840 --> 01:26:23,600
because they need purpose. Because they

2303
01:26:22,080 --> 01:26:25,120
need purpose. Yes. And struggle. They

2304
01:26:23,600 --> 01:26:27,760
need to feel they're contributing

2305
01:26:25,120 --> 01:26:29,360
something. They're useful. And do you

2306
01:26:27,760 --> 01:26:31,360
think that outcome that there's going to

2307
01:26:29,360 --> 01:26:34,560
be huge job displacement is more

2308
01:26:31,360 --> 01:26:36,400
probable than not? Yes, I do. And what

2309
01:26:34,560 --> 01:26:38,719
sort of that one I think is definitely

2310
01:26:36,400 --> 01:26:41,280
more probable than not. If I worked in a

2311
01:26:38,719 --> 01:26:42,960
call center, I'd be terrified.

2312
01:26:41,280 --> 01:26:44,400
And what's the time frame for that in

2313
01:26:42,960 --> 01:26:46,639
terms of mass jobs? I think it's

2314
01:26:44,400 --> 01:26:49,120
beginning to happen already. I read an

2315
01:26:46,639 --> 01:26:51,440
article in the Atlantic recently that

2316
01:26:49,120 --> 01:26:54,639
said it's already getting hard for

2317
01:26:51,440 --> 01:26:56,800
university graduates to get jobs. And

2318
01:26:54,639 --> 01:26:58,719
part of that may be that people are

2319
01:26:56,800 --> 01:27:02,000
already using AI for the jobs they would

2320
01:26:58,719 --> 01:27:04,080
have got. I spoke to the CEO of a major

2321
01:27:02,000 --> 01:27:07,120
company that everyone will know of, lots

2322
01:27:04,080 --> 01:27:08,560
of people use, and he said to me in DMs

2323
01:27:07,120 --> 01:27:11,280
that they used to have seven just over

2324
01:27:08,560 --> 01:27:13,760
7,000 employees. He said uh by last year

2325
01:27:11,280 --> 01:27:15,840
they were down to I think 5,000. He said

2326
01:27:13,760 --> 01:27:17,280
right now they have 3,600. And he said

2327
01:27:15,840 --> 01:27:19,679
by the end of summer because of AI

2328
01:27:17,280 --> 01:27:21,360
agents they'll be down to 3,000. So

2329
01:27:19,679 --> 01:27:23,280
you've got So it's happening already.

2330
01:27:21,360 --> 01:27:25,520
Yes. He's halfed his workforce because

2331
01:27:23,280 --> 01:27:28,000
AI agents can now handle 80% of the

2332
01:27:25,520 --> 01:27:30,639
customer service inquiries and other

2333
01:27:28,000 --> 01:27:33,840
things. So it's it's happening already.

2334
01:27:30,639 --> 01:27:36,320
Yeah. So urgent action is needed. Yep. I

2335
01:27:33,840 --> 01:27:37,920
don't know what that urgent action is.

2336
01:27:36,320 --> 01:27:40,800
That's a tricky one because that depends

2337
01:27:37,920 --> 01:27:42,480
very much on the political system and

2338
01:27:40,800 --> 01:27:44,400
political systems are all going in the

2339
01:27:42,480 --> 01:27:46,159
wrong direction at present. I mean what

2340
01:27:44,400 --> 01:27:47,600
do we need to do? Save up money? Like do

2341
01:27:46,159 --> 01:27:50,960
we save money? Do we move to another

2342
01:27:47,600 --> 01:27:53,440
part of the world? I don't know. What

2343
01:27:50,960 --> 01:27:54,400
would you tell your kids to do? They

2344
01:27:53,440 --> 01:27:56,880
said, "Dad, like there's going to be

2345
01:27:54,400 --> 01:27:58,480
loads of job displacement." Because I

2346
01:27:56,880 --> 01:28:01,840
worked for Google for 10 years. is they

2347
01:27:58,480 --> 01:28:03,440
have enough money. Okay. Okay. [ __ ] So,

2348
01:28:01,840 --> 01:28:06,000
they're not typical. What if they didn't

2349
01:28:03,440 --> 01:28:09,560
have money? Trained to be a plumber.

2350
01:28:06,000 --> 01:28:09,560
Really? Yeah.

2351
01:28:10,960 --> 01:28:15,840
Jeffrey, thank you so much. You're the

2352
01:28:12,639 --> 01:28:18,080
first Nobel Prize winner that I've ever

2353
01:28:15,840 --> 01:28:20,480
had a conversation with, I think, in my

2354
01:28:18,080 --> 01:28:22,320
life. So, that's a tremendous honor. And

2355
01:28:20,480 --> 01:28:23,840
you you you received that award for a

2356
01:28:22,320 --> 01:28:25,199
lifetime of exceptional work and pushing

2357
01:28:23,840 --> 01:28:28,080
the world forward in so many profound

2358
01:28:25,199 --> 01:28:29,440
ways that will lead to great and that

2359
01:28:28,080 --> 01:28:31,120
have led to great advancements and

2360
01:28:29,440 --> 01:28:32,800
things that matter so much to us. And

2361
01:28:31,120 --> 01:28:34,960
now you've turned this season in your

2362
01:28:32,800 --> 01:28:36,639
life to shining a light on some of your

2363
01:28:34,960 --> 01:28:40,400
own work, but also on the the the

2364
01:28:36,639 --> 01:28:42,000
broader risks of AI and how um and how

2365
01:28:40,400 --> 01:28:44,320
it might impact us adversely. And

2366
01:28:42,000 --> 01:28:47,040
there's very few people that have worked

2367
01:28:44,320 --> 01:28:48,880
inside the machine of a Google or a big

2368
01:28:47,040 --> 01:28:51,520
tech company that have contributed to

2369
01:28:48,880 --> 01:28:53,280
the field of AI that are now at the very

2370
01:28:51,520 --> 01:28:56,159
forefront of warning us against the very

2371
01:28:53,280 --> 01:28:58,719
thing that they worked upon. There are

2372
01:28:56,159 --> 01:29:01,920
actually surprising number of us now.

2373
01:28:58,719 --> 01:29:03,440
They're not as uh as public and they're

2374
01:29:01,920 --> 01:29:04,639
actually quite hard to get to have these

2375
01:29:03,440 --> 01:29:07,280
kinds of conversations because many of

2376
01:29:04,639 --> 01:29:08,800
them are still in that industry. So, you

2377
01:29:07,280 --> 01:29:10,239
know, someone who tries to contact these

2378
01:29:08,800 --> 01:29:11,760
people often and ask invites them to

2379
01:29:10,239 --> 01:29:13,760
have conversations, they often are a

2380
01:29:11,760 --> 01:29:15,679
little bit hesitant to speak openly.

2381
01:29:13,760 --> 01:29:17,520
They speak privately, but they're less

2382
01:29:15,679 --> 01:29:19,520
willing to openly because maybe maybe

2383
01:29:17,520 --> 01:29:21,199
they still have something at some sort

2384
01:29:19,520 --> 01:29:23,520
of incentives at play. I have an

2385
01:29:21,199 --> 01:29:25,520
advantage over them, which is I'm older,

2386
01:29:23,520 --> 01:29:27,280
so I'm unemployed, so I can say what I

2387
01:29:25,520 --> 01:29:28,560
Well, there you go. So, thank you for

2388
01:29:27,280 --> 01:29:30,239
doing what you do. It's a real honor and

2389
01:29:28,560 --> 01:29:32,800
please do continue to do it. Thank you.

2390
01:29:30,239 --> 01:29:35,440
Thank you so much.

2391
01:29:32,800 --> 01:29:36,960
People

2392
01:29:35,440 --> 01:29:41,360
think I'm joking when I say that, but

2393
01:29:36,960 --> 01:29:43,740
I'm not. The plumbing fish. Yeah. Yeah.

2394
01:29:41,360 --> 01:29:58,850
And plumbers are pretty well paid.

2395
01:29:43,740 --> 01:29:58,850
[Music]

2396
01:30:03,430 --> 01:30:06,709
[Music]

